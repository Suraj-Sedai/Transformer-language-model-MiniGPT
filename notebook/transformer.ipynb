{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Suraj-Sedai/Transformer-language-model-MiniGPT/blob/main/transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaa00d69"
      },
      "source": [
        "# Mini Transformer Model Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73e40c6b"
      },
      "source": [
        "## Imports and Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "a5b61987"
      },
      "outputs": [],
      "source": [
        "from math import sqrt,exp\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "'''Some helper functions'''\n",
        "\n",
        "def random_matrix(shape):\n",
        "    rows, cols = shape\n",
        "    return [\n",
        "        [random.uniform(-0.01, 0.01) for _ in range(cols)]\n",
        "        for _ in range(rows)\n",
        "    ]\n",
        "def matmul(vec, mat):\n",
        "    # vec: [m]\n",
        "    # mat: [m][n]\n",
        "    m = len(vec)\n",
        "    n = len(mat[0])\n",
        "    result = [0]*n\n",
        "\n",
        "    for col in range(n):\n",
        "        s = 0\n",
        "        for row in range(m):\n",
        "            s += vec[row] * mat[row][col]\n",
        "        result[col] = s\n",
        "\n",
        "    return result\n",
        "\n",
        "# v = [1,2,3]\n",
        "# m = [\n",
        "#     [1,0],\n",
        "#     [0,1],\n",
        "#     [1,1]\n",
        "# ]\n",
        "# print(matmul(v, m))\n",
        "\n",
        "def layer_norm(X):\n",
        "    # X = [seq_len, embed_dim]\n",
        "    output = []\n",
        "    for token_vec in X:\n",
        "        mean = sum(token_vec)/len(token_vec)\n",
        "        variance = sum((v-mean)**2 for v in token_vec)/len(token_vec)\n",
        "        std = sqrt(variance + 1e-5)\n",
        "        normalized = [(v-mean)/std for v in token_vec]\n",
        "        output.append(normalized)\n",
        "    return output\n",
        "\n",
        "def softmax(x):\n",
        "    e = np.exp(x - np.max(x))  # for numerical stability\n",
        "    return e / e.sum(axis=-1, keepdims=True)\n",
        "\n",
        "def cross_entropy_loss(logits, target_id):\n",
        "    probs = softmax(logits)\n",
        "    return -np.log(probs[target_id] + 1e-9)  # avoid log(0)\n",
        "\n",
        "def grad_cross_entropy(logits, target_id):\n",
        "    probs = softmax(logits)\n",
        "    probs[target_id] -= 1\n",
        "    return probs  # gradient w.r.t logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e3b75d7"
      },
      "source": [
        "## BPETokenizer Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ed2324d0"
      },
      "outputs": [],
      "source": [
        "class BPETokenizer:\n",
        "    def __init__(self, vocab_size=1000):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.vocab = {}      # token -> id\n",
        "        self.inv_vocab = {}  # id -> token\n",
        "        self.merges = []     # list of merge rules\n",
        "\n",
        "    def train(self, text):\n",
        "        text = text.lower()\n",
        "\n",
        "        # ---- special tokens ----\n",
        "        for special in [\"<unk>\", \"<pad>\", \" \"]:\n",
        "            if special not in self.vocab:\n",
        "                self.vocab[special] = len(self.vocab)\n",
        "\n",
        "        # ---- split into words ----\n",
        "        words = text.strip().split()\n",
        "        tokens_list = [self._word_to_chars(w) for w in words]\n",
        "\n",
        "        # ---- add unique chars ----\n",
        "        for token_list in tokens_list:\n",
        "            for t in token_list:\n",
        "                if t not in self.vocab:\n",
        "                    self.vocab[t] = len(self.vocab)\n",
        "\n",
        "        # ---- BPE loop ----\n",
        "        while len(self.vocab) < self.vocab_size:\n",
        "            pair_counts = self.get_pair_frequencies(tokens_list)\n",
        "            if not pair_counts:\n",
        "                break\n",
        "\n",
        "            best_pair = max(pair_counts, key=pair_counts.get)\n",
        "            tokens_list = self._merge_pair(tokens_list, best_pair)\n",
        "\n",
        "            new_token = best_pair[0] + best_pair[1]\n",
        "            if new_token not in self.vocab:\n",
        "                self.vocab[new_token] = len(self.vocab)\n",
        "\n",
        "            self.merges.append(best_pair)\n",
        "\n",
        "        # ---- inverse vocab ----\n",
        "        self.inv_vocab = {idx: tok for tok, idx in self.vocab.items()}\n",
        "\n",
        "\n",
        "    def _word_to_chars(self, word):\n",
        "        # turn a word into a list of char\n",
        "        return list(word)\n",
        "\n",
        "    def encode(self, text):\n",
        "        text = text.lower().strip()\n",
        "        words = text.split()\n",
        "\n",
        "        # convert each word into list of characters\n",
        "        tokens_list = [self._word_to_chars(w) for w in words]\n",
        "\n",
        "        # apply BPE merges\n",
        "        for merge_pair in self.merges:\n",
        "            tokens_list = self._merge_pair(tokens_list, merge_pair)\n",
        "\n",
        "        # convert tokens into ids (safe lookup)\n",
        "        token_ids = []\n",
        "        for token_list in tokens_list:\n",
        "            for token in token_list:\n",
        "                token_ids.append(self.vocab.get(token, self.vocab[\"<unk>\"]))\n",
        "\n",
        "        return token_ids\n",
        "\n",
        "\n",
        "    def decode(self, token_ids):\n",
        "        tokens = [self.inv_vocab.get(i, \"<unk>\") for i in token_ids]\n",
        "        text = ' '.join(tokens)\n",
        "        return text\n",
        "\n",
        "\n",
        "    def get_pair_frequencies(self, tokens_list):\n",
        "        # get frequencies of adjacent token pairs\n",
        "        pair_counts = dict()\n",
        "        for token_list in tokens_list:\n",
        "            for i in range(len(token_list)-1):\n",
        "                pair = (token_list[i], token_list[i+1])\n",
        "\n",
        "                if pair not in pair_counts:\n",
        "                    pair_counts[pair] = 1\n",
        "                else:\n",
        "                    pair_counts[pair] +=1\n",
        "        return pair_counts\n",
        "\n",
        "    def _merge_pair(self, tokens_list,pair_to_merge):\n",
        "        #pair to merge in tuple\n",
        "        a = pair_to_merge[0]\n",
        "        b = pair_to_merge[1]\n",
        "        new_tokens_list = []\n",
        "        #processing each word one by one\n",
        "        for token_list in tokens_list:\n",
        "            merged_word = []\n",
        "            i = 0\n",
        "            while i < len(token_list):\n",
        "                if i < len(token_list)-1 and token_list[i] == a and token_list[i + 1] == b:\n",
        "                    #merge two token\n",
        "                    merged_token = a+b\n",
        "                    merged_word.append(merged_token)\n",
        "                    i +=2\n",
        "                else:\n",
        "                    merged_word.append(token_list[i])\n",
        "                    i +=1\n",
        "            #add processed word back to list\n",
        "            new_tokens_list.append(merged_word)\n",
        "        return new_tokens_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary: {'<unk>': 0, '<pad>': 1, ' ': 2, 'h': 3, 'e': 4, 'l': 5, 'o': 6, 'he': 7, 'hel': 8, 'hell': 9, 'hello': 10}\n",
            "orig: 'Hello, world!  This is a test.\\nNew line.'\n",
            "back: 'hello <unk> <unk> o <unk> l <unk> <unk> <unk> h <unk> <unk> <unk> <unk> <unk> <unk> e <unk> <unk> <unk> <unk> e <unk> l <unk> <unk> e <unk>'\n"
          ]
        }
      ],
      "source": [
        "#test BPETokenizer\n",
        "\n",
        "tokenizer = BPETokenizer(vocab_size=50)\n",
        "sample_text = \"hello hello hell he hello\"\n",
        "tokenizer.train(sample_text)\n",
        "print(\"Vocabulary:\", tokenizer.vocab)\n",
        "token_ids = tokenizer.encode(\"hello hell\")\n",
        "x = \"Hello, world!  This is a test.\\nNew line.\"\n",
        "ids = tokenizer.encode(x)\n",
        "y = tokenizer.decode(ids)\n",
        "print(\"orig:\", repr(x))\n",
        "print(\"back:\", repr(y))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56c0dbb5"
      },
      "source": [
        "## KNN Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "5f7d44c4"
      },
      "outputs": [],
      "source": [
        "class KNN:\n",
        "    def __init__(self, k=3):\n",
        "        self.k = k\n",
        "        self.X = []\n",
        "        self.y = []\n",
        "\n",
        "    def fit(self,X,y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def euclidean_distance(self,point1, point2):\n",
        "        #point1 and point2\n",
        "        sum_of_square = 0\n",
        "        for i in range(len(point1)):\n",
        "\n",
        "            #compute the difference between each corresponding features\n",
        "            diff = (point1[i]-point2[i])\n",
        "            #square the difference to ensure distance is positive\n",
        "            squared = diff * diff\n",
        "\n",
        "            sum_of_square += squared\n",
        "\n",
        "        distance = sqrt(sum_of_square)\n",
        "        return distance\n",
        "\n",
        "    def get_k_nearest_neighbors(self, training_data, training_labels, new_point, k):\n",
        "        distances = []\n",
        "        for i in range (len(training_data)):\n",
        "            #get current example from the training dataset\n",
        "            current_point = training_data[i]\n",
        "\n",
        "            #calculate distance between new point and training point\n",
        "            dist = self.euclidean_distance(new_point, current_point)\n",
        "\n",
        "            #store pair\n",
        "            distances.append((dist, training_labels[i]))\n",
        "\n",
        "        #SORT DISTANCE\n",
        "        distances = sorted(distances, key=lambda x: x[0])\n",
        "        #select first k entries from sorted list\n",
        "        neighbours = [distances[i] for i in range(k)]\n",
        "\n",
        "        return neighbours\n",
        "\n",
        "    def majority_vote(self, neighbors):\n",
        "        label_count = dict()\n",
        "        for pair in neighbors:\n",
        "            label = pair[1]\n",
        "            if label not in label_count:\n",
        "                label_count[label] = 1\n",
        "            else:\n",
        "                label_count[label] += 1\n",
        "        # Find label with highest count\n",
        "\n",
        "        most_common_label = max(label_count, key=label_count.get)\n",
        "\n",
        "        return most_common_label\n",
        "\n",
        "    def predict(self, test_point):\n",
        "        distances = []\n",
        "\n",
        "        for sample, label in zip(self.X, self.y):\n",
        "            d = self.euclidean_distance(test_point, sample)\n",
        "            distances.append((d,label))\n",
        "\n",
        "        #sorting\n",
        "        distances = sorted(distances, key=lambda x: x[0])\n",
        "        #take the k nearest neighbour\n",
        "        k_neighbour = distances[:self.k]\n",
        "        #use majority vote to predict\n",
        "        predicted_label = self.majority_vote(k_neighbour)\n",
        "\n",
        "        return predicted_label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4adcaf9b"
      },
      "source": [
        "## Embedding Layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "66ba1a74"
      },
      "outputs": [],
      "source": [
        "class TokenEmbedding:\n",
        "    def __init__(self, vocab_size, embed_dim):\n",
        "        #create embedding matrix with random small values\n",
        "        self.W = random_matrix(shape=(vocab_size, embed_dim))\n",
        "\n",
        "    def forward(self, token_ids):\n",
        "        #token_ids : like [ 8,6,4,11]\n",
        "        embeddings = []\n",
        "        for id in token_ids:\n",
        "            #lookup = row from embedding matrix\n",
        "            vector = self.W[id]\n",
        "            embeddings.append(vector)\n",
        "        return embeddings\n",
        "\n",
        "class PosEmbedding:\n",
        "    def __init__(self,max_len, embed_dim):\n",
        "        self.P = random_matrix(shape=(max_len, embed_dim))\n",
        "    def forward(self, token_ids):\n",
        "        length = len(token_ids)\n",
        "        #add position bector to each token embedding\n",
        "        return [self.P[pos] for pos in range(length)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1943a58a"
      },
      "source": [
        "## SelfAttention Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "0a1f7258"
      },
      "outputs": [],
      "source": [
        "'''Implement Q, K, V Weight Matrices + Compute Q, K, V vectors'''\n",
        "class SelfAttention:\n",
        "    def __init__(self, embed_dim):\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def forward(self, Q, K, V):\n",
        "        \"\"\"\n",
        "        Q, K, V shapes: (batch, seq_len, embed_dim)\n",
        "        Return:\n",
        "            output: (batch, seq_len, embed_dim)\n",
        "            weights: (batch, seq_len, seq_len)\n",
        "        \"\"\"\n",
        "\n",
        "        B, T, D = Q.shape\n",
        "\n",
        "        # 1. Compute attention scores = QÂ·K^T\n",
        "        # shape -> (B, T, T)\n",
        "        scores = np.matmul(Q, K.transpose(0, 2, 1))\n",
        "\n",
        "        # 2. Scale\n",
        "        scores = scores / np.sqrt(D)\n",
        "\n",
        "        # 3. Softmax across last dimension\n",
        "        exp_scores = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n",
        "        weights = exp_scores / np.sum(exp_scores, axis=-1, keepdims=True)\n",
        "\n",
        "        # 4. Weighted sum over V\n",
        "        # (B, T, T) @ (B, T, D) -> (B, T, D)\n",
        "        output = np.matmul(weights, V)\n",
        "\n",
        "        return output, weights\n",
        "\n",
        "    def dot(self,a,b):\n",
        "        sum = 0\n",
        "        for i in range(len(a)):\n",
        "            sum += a[i] * b[i]\n",
        "        return sum\n",
        "\n",
        "    def soft_max(self, scores):\n",
        "        scores = np.array(scores, dtype=np.float64)   # convert to array\n",
        "        scores = scores - np.max(scores)             # numerical stability\n",
        "        exps = np.exp(scores)                         # numpy exp works on arrays\n",
        "        probs = exps / np.sum(exps, axis=-1, keepdims=True)\n",
        "        return probs\n",
        "\n",
        "    def attention_scores(self, Q, K):\n",
        "        matrix = []\n",
        "        for qi in Q:\n",
        "            row = []\n",
        "            for kj in K:\n",
        "                row.append(self.dot(qi, kj))\n",
        "            matrix.append(row)\n",
        "        return matrix\n",
        "\n",
        "    def weighted_sum(self, weights, V):\n",
        "        dim = len(V[0])\n",
        "        result = [0.0 for _ in range(dim)]\n",
        "        for i in range(len(V)):\n",
        "            for d in range(dim):\n",
        "                result[d] += weights[i] * V[i][d]\n",
        "        return result\n",
        "\n",
        "    def compute_attention(self,Q,K,V):\n",
        "        scores = self.attention_scores(Q,K)\n",
        "        # scale\n",
        "        for i in range(len(scores)):\n",
        "            for j in range(len(scores[i])):\n",
        "                scores[i][j] /= sqrt(self.embed_dim)\n",
        "        output = []\n",
        "        for row in scores:\n",
        "            w = self.soft_max(row)\n",
        "            out_vec = self.weighted_sum(w, V)\n",
        "            output.append(out_vec)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5036297e"
      },
      "source": [
        "## AttentionHead Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "6e6759f4"
      },
      "outputs": [],
      "source": [
        "class AttentionHead:\n",
        "    def __init__(self, embed_dim, head_dim):\n",
        "        self.embed_dim = embed_dim\n",
        "        self.head_dim = head_dim\n",
        "\n",
        "        self.W_q = random_matrix((embed_dim, head_dim))\n",
        "        self.W_k = random_matrix((embed_dim, head_dim))\n",
        "        self.W_v = random_matrix((embed_dim, head_dim))\n",
        "\n",
        "    # -------------------------\n",
        "    # Copy these 5 methods below\n",
        "    # -------------------------\n",
        "    def dot(self, a, b):\n",
        "        s = 0\n",
        "        for i in range(len(a)):\n",
        "            s += a[i] * b[i]\n",
        "        return s\n",
        "\n",
        "    def attention_scores(self, Q, K):\n",
        "        matrix = []\n",
        "        for qi in Q:\n",
        "            row = []\n",
        "            for kj in K:\n",
        "                row.append(self.dot(qi, kj))\n",
        "            matrix.append(row)\n",
        "        return matrix\n",
        "\n",
        "    def soft_max(self, scores):\n",
        "        scores = np.array(scores, dtype=np.float64)   # convert to array\n",
        "        scores = scores - np.max(scores)             # numerical stability\n",
        "        exps = np.exp(scores)                         # numpy exp works on arrays\n",
        "        probs = exps / np.sum(exps, axis=-1, keepdims=True)\n",
        "        return probs\n",
        "\n",
        "    def weighted_sum(self, weights, V):\n",
        "        dim = len(V[0])\n",
        "        result = [0.0 for _ in range(dim)]\n",
        "\n",
        "        for i in range(len(V)):\n",
        "            for d in range(dim):\n",
        "                result[d] += weights[i] * V[i][d]\n",
        "        return result\n",
        "\n",
        "    def compute_attention(self, Q, K, V):\n",
        "        scores = self.attention_scores(Q, K)\n",
        "        for i in range(len(scores)):\n",
        "            for j in range(len(scores[i])):\n",
        "                scores[i][j] /= sqrt(self.head_dim)\n",
        "\n",
        "        output = []\n",
        "        for row in scores:\n",
        "            w = self.soft_max(row)\n",
        "            out_vec = self.weighted_sum(w, V)\n",
        "            output.append(out_vec)\n",
        "        return output\n",
        "\n",
        "    # -------------------------\n",
        "    # Main forward\n",
        "    # -------------------------\n",
        "    def forward(self, X):\n",
        "        Q, K, V = [], [], []\n",
        "        for token_vec in X:\n",
        "            Q.append(matmul(token_vec, self.W_q))\n",
        "            K.append(matmul(token_vec, self.W_k))\n",
        "            V.append(matmul(token_vec, self.W_v))\n",
        "\n",
        "        return self.compute_attention(Q, K, V)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8ea2580"
      },
      "source": [
        "## MultiHeadAttention Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "1851d2f7"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention:\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        self.heads = [ AttentionHead(embed_dim, self.head_dim) for _ in range(num_heads) ]\n",
        "\n",
        "        # ðŸ”¥ Final linear layer after concatenation\n",
        "        self.Wo = random_matrix((embed_dim, embed_dim))\n",
        "\n",
        "    def concat_along_last_dim(self, head_outputs):\n",
        "        # head_outputs: list of [seq_len x head_dim] arrays\n",
        "        seq_len = len(head_outputs[0])\n",
        "        num_heads = len(head_outputs)\n",
        "        head_dim = len(head_outputs[0][0])\n",
        "\n",
        "        # initialize result\n",
        "        result = []\n",
        "\n",
        "        # loop over tokens\n",
        "        for i in range(seq_len):\n",
        "            concatenated = []\n",
        "            for head in head_outputs:\n",
        "                concatenated.extend(head[i])  # append head vector for token i\n",
        "            result.append(concatenated)\n",
        "\n",
        "        return result\n",
        "\n",
        "    def forward(self, X):\n",
        "        # X shape: [seq_len, embed_dim]\n",
        "\n",
        "        head_outputs = []\n",
        "\n",
        "        for head in self.heads:\n",
        "            # for each head, run attention\n",
        "            # BUT: X must first be projected down to head_dim\n",
        "\n",
        "            # Create Q,K,V using head weight matrices\n",
        "            # (selfattention already does this)\n",
        "\n",
        "            out = head.forward(X)\n",
        "            # shape: [seq_len, head_dim]\n",
        "\n",
        "            head_outputs.append(out)\n",
        "\n",
        "        # concatenate outputs from all heads\n",
        "        # final shape: [seq_len, embed_dim]\n",
        "        concatenated = self.concat_along_last_dim(head_outputs)\n",
        "\n",
        "        # final linear projection\n",
        "        final_output = []\n",
        "        for vec in concatenated:\n",
        "            final_output.append(matmul(vec, self.Wo))\n",
        "\n",
        "        return final_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8a4d632"
      },
      "source": [
        "## FeedForward and LayerNorm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "2e18a03b"
      },
      "outputs": [],
      "source": [
        "def zeros(n):\n",
        "    return [0.0 for _ in range(n)]\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(x, 0)\n",
        "\n",
        "\n",
        "def add_vectors(a, b):\n",
        "    \"\"\"Element-wise add two vectors (same length).\"\"\"\n",
        "    return [a[i] + b[i] for i in range(len(a))]\n",
        "def add_vectors_list(A, B):\n",
        "    return [add_vectors(A[i], B[i]) for i in range(len(A))]\n",
        "class FeedForward:\n",
        "    def __init__(self, embed_dim, hidden_dim):\n",
        "        self.W1 = np.random.uniform(-0.1, 0.1, (embed_dim, hidden_dim))\n",
        "        self.b1 = np.zeros(hidden_dim)\n",
        "\n",
        "        self.W2 = np.random.uniform(-0.1, 0.1, (hidden_dim, embed_dim))\n",
        "        self.b2 = np.zeros(embed_dim)\n",
        "\n",
        "\n",
        "    def forward(self, X):\n",
        "        # X shape: (B, T, D)\n",
        "        hidden = np.matmul(X, self.W1) + self.b1\n",
        "        hidden = np.maximum(hidden, 0)   # ReLU\n",
        "        out = np.matmul(hidden, self.W2) + self.b2\n",
        "        return out\n",
        "\n",
        "\n",
        "class LayerNorm:\n",
        "    def __init__(self, dim, eps=1e-5):\n",
        "        self.dim = dim\n",
        "        self.eps = eps\n",
        "        # Learnable parameters (gamma, beta)\n",
        "        self.gamma = [1.0] * dim\n",
        "        self.beta = [0.0] * dim\n",
        "\n",
        "    def forward(self, X):\n",
        "        # X is list of vectors: [seq_len][dim]\n",
        "        out = []\n",
        "        for vec in X:\n",
        "            mean = sum(vec) / self.dim\n",
        "            var = sum((v - mean)**2 for v in vec) / self.dim\n",
        "            std = (var + self.eps) ** 0.5\n",
        "\n",
        "            norm = [ (vec[i] - mean) / std for i in range(len(vec)) ]\n",
        "            out.append([ norm[i] * self.gamma[i] + self.beta[i] for i in range(len(vec)) ])\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cf9bf4b"
      },
      "source": [
        "## TransformerBlock Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "6c562d5c"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock:\n",
        "    def __init__(self, embed_dim, num_heads, ff_hidden_dim):\n",
        "        self.ln1 = LayerNorm(embed_dim)\n",
        "        self.mha = MultiHeadAttention(embed_dim, num_heads)\n",
        "        self.ln2 = LayerNorm(embed_dim)\n",
        "        self.ff = FeedForward(embed_dim, ff_hidden_dim)\n",
        "\n",
        "    def forward(self, X):\n",
        "        # if batch dim exists, drop it for simplicity\n",
        "        if isinstance(X, np.ndarray) and X.ndim == 3:\n",
        "            X = X[0].tolist()  # shape -> [seq_len, embed_dim]\n",
        "\n",
        "        # LayerNorm + MultiHeadAttention\n",
        "        normed = self.ln1.forward(X)\n",
        "        attn_out = self.mha.forward(normed)  # shape: [seq_len, embed_dim]\n",
        "\n",
        "        # Residual\n",
        "        x2 = add_vectors_list(X, attn_out)\n",
        "\n",
        "        # LayerNorm + FeedForward\n",
        "        normed2 = self.ln2.forward(x2)\n",
        "        ff_out = self.ff.forward(normed2)\n",
        "\n",
        "        # Residual\n",
        "        out = add_vectors_list(x2, ff_out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac89fc71"
      },
      "source": [
        "## TransformerModel Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "840448c1"
      },
      "outputs": [],
      "source": [
        "class TransformerModel:\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, ffn_hidden_dim, max_len=128):\n",
        "        self.token_embed = TokenEmbedding(vocab_size, embed_dim)\n",
        "        self.pos_embed = PosEmbedding(max_len, embed_dim)\n",
        "        self.blocks = [TransformerBlock(embed_dim, num_heads, ffn_hidden_dim)\n",
        "                       for _ in range(num_layers)]\n",
        "        self.Wo = random_matrix((embed_dim, vocab_size))\n",
        "        self.bo = np.zeros(vocab_size)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.max_len = max_len\n",
        "\n",
        "\n",
        "    def forward(self, ids):\n",
        "        # ids: list of token IDs\n",
        "        tok_vecs = self.token_embed.forward(ids)\n",
        "        pos_vecs = self.pos_embed.forward(ids)\n",
        "        X = [add_vectors(tok_vecs[i], pos_vecs[i]) for i in range(len(ids))]  # seq_len x embed_dim\n",
        "\n",
        "        # pass through blocks\n",
        "        for block in self.blocks:\n",
        "            X = block.forward(X)  # output: seq_len x embed_dim\n",
        "\n",
        "        # final linear layer\n",
        "        logits = np.matmul(X, self.Wo) + self.bo  # seq_len x vocab_size\n",
        "        return logits\n",
        "\n",
        "    def generate(self, idx, max_new_tokens, tokenizer):\n",
        "        \"\"\"\n",
        "        idx: list of token ids (context)\n",
        "        max_new_tokens: how many tokens to generate\n",
        "        tokenizer: your BPE or simple tokenizer\n",
        "        \"\"\"\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            # forward pass: logits shape (T, vocab)\n",
        "            logits = self.forward(idx)\n",
        "\n",
        "            # take last position\n",
        "            last_logits = logits[-1]   # shape: (vocab,)\n",
        "\n",
        "            # convert to probabilities using softmax\n",
        "            exps = np.exp(last_logits - np.max(last_logits))\n",
        "            probs = exps / np.sum(exps)\n",
        "\n",
        "            # sample from distribution (probabilistic)\n",
        "            next_id = int(np.random.choice(len(probs), p=probs))\n",
        "\n",
        "\n",
        "            # append prediction\n",
        "            idx.append(next_id)\n",
        "\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c280e69"
      },
      "source": [
        "## MiniTransformer Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "abf4885c"
      },
      "outputs": [],
      "source": [
        "class MiniTransformer:\n",
        "    def __init__(self, vocab_size, max_len, embed_dim, num_heads, ff_hidden_dim, num_layers):\n",
        "        self.token_embed = TokenEmbedding(vocab_size, embed_dim)\n",
        "        self.pos_embed = PosEmbedding(max_len, embed_dim)\n",
        "\n",
        "        self.blocks = [\n",
        "            TransformerBlock(embed_dim, num_heads, ff_hidden_dim)\n",
        "            for _ in range(num_layers)\n",
        "        ]\n",
        "\n",
        "    def forward(self, token_ids):\n",
        "        tok = self.token_embed.forward(token_ids)\n",
        "        pos = self.pos_embed.forward(token_ids)\n",
        "\n",
        "        X = add_vectors_list(tok, pos)\n",
        "\n",
        "        for block in self.blocks:\n",
        "            X = block.forward(X)\n",
        "\n",
        "        return X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "782e94c4"
      },
      "source": [
        "## Example Usage: Tokenizer Training and Model Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "ea64aa00"
      },
      "outputs": [],
      "source": [
        "# Example tiny corpus\n",
        "text = \"hello world hello transformer model mini gpt\"\n",
        "\n",
        "# Initialize and train tokenizer\n",
        "tokenizer = BPETokenizer(vocab_size=50)\n",
        "tokenizer.train(text)\n",
        "\n",
        "# Convert text to token IDs\n",
        "token_ids = tokenizer.encode(text)  # e.g., [1, 5, 1, 20, 3, ...]\n",
        "seq_len = 4  # number of tokens in input\n",
        "X_train = []\n",
        "y_train = []\n",
        "\n",
        "for i in range(len(token_ids) - seq_len):\n",
        "    X_train.append(token_ids[i:i+seq_len])\n",
        "    y_train.append(token_ids[i+seq_len])\n",
        "\n",
        "X_train = np.array(X_train)  # shape: (num_samples, seq_len)\n",
        "y_train = np.array(y_train)  # shape: (num_samples,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "952cd46a"
      },
      "source": [
        "## Model Initialization and Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf39f6e5",
        "outputId": "2b0508b2-cfbe-43ab-b030-3d9e25ebcece"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, loss: 1.1182\n",
            "Epoch 10, loss: 0.8621\n",
            "Epoch 20, loss: 0.6495\n",
            "Epoch 30, loss: 0.4971\n",
            "Epoch 40, loss: 0.4010\n",
            "Epoch 50, loss: 0.3432\n",
            "Epoch 60, loss: 0.3076\n",
            "Epoch 70, loss: 0.2846\n",
            "Epoch 80, loss: 0.2688\n",
            "Epoch 90, loss: 0.2575\n",
            "Epoch 100, loss: 0.2491\n",
            "Epoch 110, loss: 0.2425\n",
            "Epoch 120, loss: 0.2373\n",
            "Epoch 130, loss: 0.2331\n",
            "Epoch 140, loss: 0.2295\n",
            "Epoch 150, loss: 0.2265\n",
            "Epoch 160, loss: 0.2238\n",
            "Epoch 170, loss: 0.2216\n",
            "Epoch 180, loss: 0.2195\n",
            "Epoch 190, loss: 0.2177\n"
          ]
        }
      ],
      "source": [
        "lr = 0.01  # learning rate\n",
        "model = TransformerModel(vocab_size=tokenizer.vocab_size, embed_dim=16,\n",
        "                         num_heads=2, num_layers=2, ffn_hidden_dim=32)\n",
        "\n",
        "epochs = 100\n",
        "import numpy as np\n",
        "\n",
        "# --- Hyperparameters ---\n",
        "lr = 0.1        # learning rate\n",
        "epochs = 200\n",
        "seq_len = 5     # small sequence length for tiny corpus\n",
        "\n",
        "# Assume you already have:\n",
        "# model: TransformerModel instance\n",
        "# tokenizer: BPETokenizer instance\n",
        "# text: training text (string)\n",
        "\n",
        "# Encode the text into token IDs\n",
        "ids = tokenizer.encode(text)\n",
        "\n",
        "# Training loop (simple next-token prediction)\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0.0\n",
        "    for i in range(len(ids) - seq_len):\n",
        "        x_ids = ids[i:i+seq_len]\n",
        "        y_id = ids[i+seq_len]\n",
        "\n",
        "        logits = model.forward(x_ids)\n",
        "        pred = logits[-1]  # last token logits\n",
        "\n",
        "        # softmax & loss\n",
        "        exp_pred = np.exp(pred - np.max(pred))\n",
        "        probs = exp_pred / np.sum(exp_pred)\n",
        "        loss = -np.log(probs[y_id] + 1e-8)\n",
        "        total_loss += loss\n",
        "\n",
        "        # gradient for Wo & bo\n",
        "        grad = probs.copy()\n",
        "        grad[y_id] -= 1.0\n",
        "\n",
        "        # last hidden vector\n",
        "        X_block = model.blocks[-1].forward(\n",
        "            [add_vectors(model.token_embed.forward(x_ids)[-1],\n",
        "                         model.pos_embed.forward(x_ids)[-1])]\n",
        "        )\n",
        "        h = np.array(X_block[-1])\n",
        "\n",
        "        # update\n",
        "        model.Wo -= lr * np.outer(h, grad)\n",
        "        model.bo -= lr * grad\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch}, loss: {total_loss / len(ids):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca5a984b"
      },
      "source": [
        "## Text Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79566418",
        "outputId": "0c2bd399-27b5-4332-bb55-66524be7873d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hello world gpt mini gpt mini gpt gpt gpt mini gpt gpt mini gpt gpt gpt gpt i gpt mini gpt gpt\n"
          ]
        }
      ],
      "source": [
        "prompt = \"hello world\"\n",
        "ids = tokenizer.encode(prompt)\n",
        "generated_ids = model.generate(ids, max_new_tokens=20, tokenizer=tokenizer)\n",
        "print(tokenizer.decode(generated_ids))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f45b1498"
      },
      "source": [
        "## Test Cases (Commented Out in Original Code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8c993645",
        "outputId": "b574889e-430e-472e-fc1c-dad54bf712e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A\n",
            "B\n",
            "{'<unk>': 0, '<pad>': 1, ' ': 2, 't': 3, 'h': 4, 'i': 5, 's': 6, 'a': 7, 'e': 8, '.': 9, 'f': 10, 'u': 11, 'n': 12, 'is': 13, 'th': 14, 'this': 15, 'te': 16, 'tes': 17, 'test': 18, 'test.': 19, 'fu': 20, 'fun': 21, 'fun.': 22}\n",
            "[('i', 's'), ('t', 'h'), ('th', 'is'), ('t', 'e'), ('te', 's'), ('tes', 't'), ('test', '.'), ('f', 'u'), ('fu', 'n'), ('fun', '.')]\n",
            "[11, 9, 7, 14]\n",
            "[[-0.0038562666512654232, 0.013543435439029604, 0.002386139174438924, 0.0014017045080263798, -0.007295962005154708, -0.004746168676615587, 0.00835282021000269, -0.009057551144605745, -0.003834717210966722, 0.0018884239541427017, -0.0076352700975058935, 0.011606513040123403, -0.0013874650087399194, -0.0020608745639878944, 5.7147458480120636e-05, -0.011570913349644137, 0.007828019023602539, 0.015884691118077346, -0.006529719527763926, 0.006383753990881955, -0.006566338254676601, 0.004507998086054276, 0.005556244247505495, -0.015883806432631126, -0.0053079142738368355, 0.009765047442905746, -0.012840908445004262, 0.008064082897768476, 0.0022982983359617513, -0.0035893519338718143, 0.0009742188348203021, 0.001702890452569555], [0.009070383950145334, -0.011909905662909927, 0.008319795359432735, -0.018344640223373655, -0.004475997464447858, -0.002866328942603566, -0.00396372739056766, -0.011790984454514035, -0.012916219063270146, 0.0022102958878144367, 0.003965199265902625, -0.0017690564441251461, 0.0028420176711578937, 0.013116431133922636, 0.009835234331709984, -0.004340931528962521, -0.012509713359084112, 0.005344011576227821, 0.01295832457112297, -0.0001127196746953038, -0.01224234896666338, 0.0012344657715015256, 0.0028485664376446448, -0.008533910401508749, 0.0014740838126102258, -0.0038841431625605037, 0.008826271273277972, 0.008653377994047012, -0.006562579056457363, 0.00013726843932322132, 0.004907683044528375, -0.0052825848213566215], [0.015883496370021313, -0.00018190945635832186, -0.018898113946231483, 0.008497561566317072, 0.0015769612700942673, 0.013385781440057856, -0.002578961007184596, 0.0066580465791627914, -0.0058674905119767366, 0.0058291595606612435, -0.007268599431783087, -0.0034913365797317143, 0.004939783901754308, 0.0163099483206778, 0.011140082446321257, 0.005551844145332024, 0.002015736933970765, -0.0055371586496854065, 0.001986916287569142, 0.0003378279704007621, -0.0044706416310180425, -0.005051303863404757, 0.006703638584783003, -0.016321685327133337, 0.011220669573840459, -0.007937384699705974, -0.00910615592755501, 0.00929649327099171, -0.004331376330710483, -0.0015934637922213621, -0.002666689151754449, 0.0035896848099753256], [0.003022306509095781, 0.0029527612432294244, 0.018456225921994595, 4.72671836520425e-05, 0.008766073161457283, -0.011509026305580488, -0.0005210943660086138, -0.007075665251419051, 0.006137256926321784, 0.001006028724411575, 0.005861217389035139, 0.004429391723928277, -0.003677135661336382, -0.00875977024976609, -0.001472243147035172, -0.006224627031489778, 0.0070425433195585845, 0.001397413138261079, 0.0014395112921045871, -0.00335673598574494, 0.0014067314804076126, -0.011981133232948528, -0.007818034426588325, -0.011277816674973847, 0.0013614414485053983, 0.000664743665379313, -0.01183013312373924, 0.0023157314795833354, -0.01202822766433191, 0.0026541342924694556, -0.01027496980806986, -0.01015207895843865]]\n",
            "Output shape: (1, 2, 4)\n",
            "Weights shape: (1, 2, 2)\n",
            "block out shape: 4 4\n",
            "[[np.float64(0.01461135676873487), np.float64(-0.014146738046641601), np.float64(0.019974809948970815), np.float64(0.009030405148297064)], [np.float64(0.025129453608062347), np.float64(0.006162830323776407), np.float64(-0.007483750130544626), np.float64(-0.012844650690160631)], [np.float64(0.000978971835459498), np.float64(-0.005228943257066574), np.float64(-0.0032938974759672375), np.float64(-0.006933342004737757)], [np.float64(0.00020781788096048154), np.float64(0.01035403320747938), np.float64(0.02738648787470998), np.float64(-0.010704232687546984)]]\n",
            "block out shape: 4 4\n",
            "[[np.float64(0.01461135676873487), np.float64(-0.014146738046641601), np.float64(0.019974809948970815), np.float64(0.009030405148297064)], [np.float64(0.025129453608062347), np.float64(0.006162830323776407), np.float64(-0.007483750130544626), np.float64(-0.012844650690160631)], [np.float64(0.000978971835459498), np.float64(-0.005228943257066574), np.float64(-0.0032938974759672375), np.float64(-0.006933342004737757)], [np.float64(0.00020781788096048154), np.float64(0.01035403320747938), np.float64(0.02738648787470998), np.float64(-0.010704232687546984)]]\n",
            "Testing full TransformerBlock...\n",
            "block out shape: 4 4\n",
            "[[np.float64(-0.020540732110820803), np.float64(-0.011980960128766876), np.float64(0.008190146623755702), np.float64(0.0004064369184002036)], [np.float64(-0.01708266865938883), np.float64(0.014652588441119847), np.float64(0.024942866436836783), np.float64(-0.0036953256480233647)], [np.float64(-0.037326671244841), np.float64(-0.035134278913937826), np.float64(-0.007209335329888593), np.float64(-0.01206915125955784)], [np.float64(0.003533176016745984), np.float64(0.00206980460910683), np.float64(0.03317891637930524), np.float64(0.008432315254962047)]]\n",
            "mha out shape: 4 4\n",
            "logits shape: 5 200\n",
            "hello tr mode f there trans ing <pad> s w th dataset model or ho this mode dataset e e tiny\n"
          ]
        }
      ],
      "source": [
        "'''Test cases for all the clasees and functions'''\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    X_knn = [\n",
        "        [1, 2],\n",
        "        [2, 3],\n",
        "        [3, 3],\n",
        "        [8, 7],\n",
        "        [9, 8],\n",
        "        [10, 8]\n",
        "    ]\n",
        "\n",
        "    y = [\"A\", \"A\", \"A\", \"B\", \"B\", \"B\"]\n",
        "\n",
        "    knn = KNN(k=3)\n",
        "    knn.fit(X_knn, y)\n",
        "\n",
        "    print(knn.predict([2, 2]))   # should give \"A\"\n",
        "    print(knn.predict([9, 7]))   # should give \"B\"\n",
        "\n",
        "\n",
        "    text = \"this is a test. this test is fun.\"\n",
        "    tokenizer = BPETokenizer(vocab_size=50)\n",
        "    tokenizer.train(text)\n",
        "    print(tokenizer.vocab)\n",
        "    print(tokenizer.merges)\n",
        "\n",
        "    text = \"this is a test\"\n",
        "    tokenizer = BPETokenizer(vocab_size=50)\n",
        "    tokenizer.train(text)\n",
        "\n",
        "    ids = tokenizer.encode(\"this is a test\")\n",
        "    print(ids)\n",
        "\n",
        "    decoded_text = tokenizer.decode(ids)\n",
        "    # print(decoded_text)\n",
        "\n",
        "\n",
        "\n",
        "    def add_vectors(a,b):\n",
        "        return [a[i] + b[i] for i in range(len(a))]\n",
        "\n",
        "    token_embed = TokenEmbedding(vocab_size=1000, embed_dim=32)\n",
        "    pos_embed = PosEmbedding(max_len=512, embed_dim=32)\n",
        "\n",
        "    ids = [8,6,4,11]\n",
        "\n",
        "    token_vectors = token_embed.forward(ids)\n",
        "    pos_vectors   = pos_embed.forward(ids)\n",
        "\n",
        "    final_vectors = [\n",
        "        add_vectors(token_vectors[i], pos_vectors[i])\n",
        "        for i in range(len(ids))\n",
        "    ]\n",
        "\n",
        "    print(final_vectors)\n",
        "\n",
        "    #Fake tiny example for SelfAttention\n",
        "    embed_dim = 4\n",
        "    X_attention_input = [\n",
        "        [0.1, 0.2, 0.3, 0.4],\n",
        "        [0.5, 0.4, 0.3, 0.2]\n",
        "    ]\n",
        "\n",
        "    att = SelfAttention(embed_dim)\n",
        "\n",
        "    # Convert X_attention_input to a numpy array with a batch dimension\n",
        "    # Expected shape for Q, K, V in SelfAttention.forward is (batch, seq_len, embed_dim)\n",
        "    X_np_for_attention = np.array(X_attention_input).reshape(1, len(X_attention_input), embed_dim)\n",
        "\n",
        "    # Pass X_np_for_attention for Q, K, and V as a simple test for self-attention\n",
        "    output_att, weights_att = att.forward(X_np_for_attention, X_np_for_attention, X_np_for_attention)\n",
        "\n",
        "    print(\"Output shape:\", output_att.shape)\n",
        "    print(\"Weights shape:\", weights_att.shape)\n",
        "\n",
        "    # ---- BUILD INPUT X ----\n",
        "    tokenizer = BPETokenizer(vocab_size=1000)\n",
        "    tokenizer.train(\"this is a test corpus for building tiny gpt tokenizer\")\n",
        "    token_embedding = TokenEmbedding(vocab_size=1000, embed_dim=4)\n",
        "    pos_embedding   = PosEmbedding(max_len=50, embed_dim=4)\n",
        "\n",
        "    ids = tokenizer.encode(\"this is a test\")\n",
        "\n",
        "    token_embed = token_embedding.forward(ids)\n",
        "    pos_embed = pos_embedding.forward(ids)\n",
        "\n",
        "    # Add token+pos embeddings\n",
        "    X_transformer_block = []\n",
        "    for i in range(len(token_embed)):\n",
        "        vec = []\n",
        "        for a, b in zip(token_embed[i], pos_embed[i]):\n",
        "            vec.append(a + b)\n",
        "        X_transformer_block.append(vec)\n",
        "\n",
        "    # ---- RUN TRANSFORMER BLOCK ----\n",
        "    tb = TransformerBlock(embed_dim=4, num_heads=2, ff_hidden_dim=16)\n",
        "    out = tb.forward(X_transformer_block)\n",
        "\n",
        "    print(\"block out shape:\", len(out), len(out[0]))\n",
        "    print(out)\n",
        "\n",
        "    print(\"block out shape:\", len(out), len(out[0]))\n",
        "    print(out)\n",
        "\n",
        "\n",
        "    print(\"Testing full TransformerBlock...\")\n",
        "    tb = TransformerBlock(embed_dim=4, num_heads=2, ff_hidden_dim=16)\n",
        "\n",
        "    out = tb.forward(X_transformer_block)\n",
        "    print(\"block out shape:\", len(out), len(out[0]))\n",
        "    print(out)\n",
        "\n",
        "    print(\"mha out shape:\", len(out), len(out[0]))  # expect seq_len x embed_dim\n",
        "\n",
        "    # small test X (seq_len=4, embed_dim must match your token/embed dims)\n",
        "\n",
        "\n",
        "    embed_dim = 4\n",
        "    num_heads = 2  # head_dim = 2\n",
        "    token_embedding = TokenEmbedding(vocab_size=100, embed_dim=embed_dim)\n",
        "    pos_embedding = PosEmbedding(max_len=20, embed_dim=embed_dim)\n",
        "    tokenizer = BPETokenizer(vocab_size=100)\n",
        "    tokenizer.train(\"this is a test\")   # small corpus ok\n",
        "\n",
        "    ids = tokenizer.encode(\"this is a test\")\n",
        "    token_vectors = token_embedding.forward(ids)\n",
        "    pos_vectors   = pos_embedding.forward(ids)\n",
        "    X_mha = [[a+b for a,b in zip(token_vectors[i], pos_vectors[i])] for i in range(len(ids))]\n",
        "\n",
        "    mha = MultiHeadAttention(embed_dim=embed_dim, num_heads=num_heads)\n",
        "    out = mha.forward(X_mha)\n",
        "    tb = TransformerBlock(embed_dim=4, num_heads=2, ff_hidden_dim=16)\n",
        "    out = tb.forward(X_mha)\n",
        "\n",
        "    model = TransformerModel(vocab_size=200, max_len=32, embed_dim=8, num_heads=2, num_layers=2, ffn_hidden_dim=32)\n",
        "    tokenizer = BPETokenizer(vocab_size=200)\n",
        "    tokenizer.train(\"this is a tiny corpus for testing\")\n",
        "    ids = tokenizer.encode(\"this is a test\")\n",
        "    logits = model.forward(ids)\n",
        "    print(\"logits shape:\", len(logits), len(logits[0]))   # expect seq_len x vocab_size\n",
        "\n",
        "    training_text = \"\"\"\n",
        "    hello world this is a tiny training dataset\n",
        "    hello there how are you\n",
        "    i am building a tiny transformer language model\n",
        "    \"\"\"\n",
        "\n",
        "    tokenizer = BPETokenizer(vocab_size=1000)\n",
        "    tokenizer.train(training_text)\n",
        "\n",
        "    # ACTUAL vocab size after training\n",
        "    vocab_size = len(tokenizer.vocab)\n",
        "\n",
        "    model = TransformerModel(vocab_size, embed_dim=64, num_heads=4, num_layers=2, ffn_hidden_dim=128)\n",
        "\n",
        "    prompt = \"hello\"\n",
        "    ids = tokenizer.encode(prompt)\n",
        "\n",
        "    generated = model.generate(ids, max_new_tokens=20, tokenizer=tokenizer)\n",
        "\n",
        "    print(tokenizer.decode(generated))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPL3rQLi20NCOthJkzlR6pO",
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
