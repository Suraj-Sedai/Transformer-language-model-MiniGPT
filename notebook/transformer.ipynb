{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Suraj-Sedai/Transformer-language-model-MiniGPT/blob/main/transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaa00d69"
      },
      "source": [
        "# Mini Transformer Model Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73e40c6b"
      },
      "source": [
        "## Imports and Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "a5b61987"
      },
      "outputs": [],
      "source": [
        "from math import sqrt,exp\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "'''Some helper functions'''\n",
        "\n",
        "def random_matrix(shape):\n",
        "    rows, cols = shape\n",
        "    return [\n",
        "        [random.uniform(-0.01, 0.01) for _ in range(cols)]\n",
        "        for _ in range(rows)\n",
        "    ]\n",
        "def matmul(vec, mat):\n",
        "    # vec: [m]\n",
        "    # mat: [m][n]\n",
        "    m = len(vec)\n",
        "    n = len(mat[0])\n",
        "    result = [0]*n\n",
        "\n",
        "    for col in range(n):\n",
        "        s = 0\n",
        "        for row in range(m):\n",
        "            s += vec[row] * mat[row][col]\n",
        "        result[col] = s\n",
        "\n",
        "    return result\n",
        "\n",
        "# v = [1,2,3]\n",
        "# m = [\n",
        "#     [1,0],\n",
        "#     [0,1],\n",
        "#     [1,1]\n",
        "# ]\n",
        "# print(matmul(v, m))\n",
        "\n",
        "def layer_norm(X):\n",
        "    # X = [seq_len, embed_dim]\n",
        "    output = []\n",
        "    for token_vec in X:\n",
        "        mean = sum(token_vec)/len(token_vec)\n",
        "        variance = sum((v-mean)**2 for v in token_vec)/len(token_vec)\n",
        "        std = sqrt(variance + 1e-5)\n",
        "        normalized = [(v-mean)/std for v in token_vec]\n",
        "        output.append(normalized)\n",
        "    return output\n",
        "\n",
        "def softmax(x):\n",
        "    e = np.exp(x - np.max(x))  # for numerical stability\n",
        "    return e / e.sum(axis=-1, keepdims=True)\n",
        "\n",
        "def cross_entropy_loss(logits, target_id):\n",
        "    probs = softmax(logits)\n",
        "    return -np.log(probs[target_id] + 1e-9)  # avoid log(0)\n",
        "\n",
        "def grad_cross_entropy(logits, target_id):\n",
        "    probs = softmax(logits)\n",
        "    probs[target_id] -= 1\n",
        "    return probs  # gradient w.r.t logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e3b75d7"
      },
      "source": [
        "## BPETokenizer Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ed2324d0"
      },
      "outputs": [],
      "source": [
        "class BPETokenizer:\n",
        "    def __init__(self, vocab_size=1000):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.vocab = {}      # token -> id\n",
        "        self.inv_vocab = {}  # id -> token\n",
        "        self.merges = []     # list of merge rules\n",
        "\n",
        "    def train(self, text):\n",
        "        text = text.lower()\n",
        "\n",
        "        # ---- special tokens ----\n",
        "        for special in [\"<unk>\", \"<pad>\", \" \"]:\n",
        "            if special not in self.vocab:\n",
        "                self.vocab[special] = len(self.vocab)\n",
        "\n",
        "        # ---- split into words ----\n",
        "        words = text.strip().split()\n",
        "        tokens_list = [self._word_to_chars(w) for w in words]\n",
        "\n",
        "        # ---- add unique chars ----\n",
        "        for token_list in tokens_list:\n",
        "            for t in token_list:\n",
        "                if t not in self.vocab:\n",
        "                    self.vocab[t] = len(self.vocab)\n",
        "\n",
        "        # ---- BPE loop ----\n",
        "        while len(self.vocab) < self.vocab_size:\n",
        "            pair_counts = self.get_pair_frequencies(tokens_list)\n",
        "            if not pair_counts:\n",
        "                break\n",
        "\n",
        "            best_pair = max(pair_counts, key=pair_counts.get)\n",
        "            tokens_list = self._merge_pair(tokens_list, best_pair)\n",
        "\n",
        "            new_token = best_pair[0] + best_pair[1]\n",
        "            if new_token not in self.vocab:\n",
        "                self.vocab[new_token] = len(self.vocab)\n",
        "\n",
        "            self.merges.append(best_pair)\n",
        "\n",
        "        # ---- inverse vocab ----\n",
        "        self.inv_vocab = {idx: tok for tok, idx in self.vocab.items()}\n",
        "\n",
        "\n",
        "    def _word_to_chars(self, word):\n",
        "        # turn a word into a list of char\n",
        "        return list(word)\n",
        "\n",
        "    def encode(self, text):\n",
        "        text = text.lower().strip()\n",
        "        words = text.split()\n",
        "\n",
        "        # convert each word into list of characters\n",
        "        tokens_list = [self._word_to_chars(w) for w in words]\n",
        "\n",
        "        # apply BPE merges\n",
        "        for merge_pair in self.merges:\n",
        "            tokens_list = self._merge_pair(tokens_list, merge_pair)\n",
        "\n",
        "        # convert tokens into ids (safe lookup)\n",
        "        token_ids = []\n",
        "        for token_list in tokens_list:\n",
        "            for token in token_list:\n",
        "                token_ids.append(self.vocab.get(token, self.vocab[\"<unk>\"]))\n",
        "\n",
        "        return token_ids\n",
        "\n",
        "\n",
        "    def decode(self, token_ids):\n",
        "        tokens = [self.inv_vocab.get(i, \"<unk>\") for i in token_ids]\n",
        "        text = ' '.join(tokens)\n",
        "        return text\n",
        "\n",
        "\n",
        "    def get_pair_frequencies(self, tokens_list):\n",
        "        # get frequencies of adjacent token pairs\n",
        "        pair_counts = dict()\n",
        "        for token_list in tokens_list:\n",
        "            for i in range(len(token_list)-1):\n",
        "                pair = (token_list[i], token_list[i+1])\n",
        "\n",
        "                if pair not in pair_counts:\n",
        "                    pair_counts[pair] = 1\n",
        "                else:\n",
        "                    pair_counts[pair] +=1\n",
        "        return pair_counts\n",
        "\n",
        "    def _merge_pair(self, tokens_list,pair_to_merge):\n",
        "        #pair to merge in tuple\n",
        "        a = pair_to_merge[0]\n",
        "        b = pair_to_merge[1]\n",
        "        new_tokens_list = []\n",
        "        #processing each word one by one\n",
        "        for token_list in tokens_list:\n",
        "            merged_word = []\n",
        "            i = 0\n",
        "            while i < len(token_list):\n",
        "                if i < len(token_list)-1 and token_list[i] == a and token_list[i + 1] == b:\n",
        "                    #merge two token\n",
        "                    merged_token = a+b\n",
        "                    merged_word.append(merged_token)\n",
        "                    i +=2\n",
        "                else:\n",
        "                    merged_word.append(token_list[i])\n",
        "                    i +=1\n",
        "            #add processed word back to list\n",
        "            new_tokens_list.append(merged_word)\n",
        "        return new_tokens_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56c0dbb5"
      },
      "source": [
        "## KNN Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5f7d44c4"
      },
      "outputs": [],
      "source": [
        "class KNN:\n",
        "    def __init__(self, k=3):\n",
        "        self.k = k\n",
        "        self.X = []\n",
        "        self.y = []\n",
        "\n",
        "    def fit(self,X,y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def euclidean_distance(self,point1, point2):\n",
        "        #point1 and point2\n",
        "        sum_of_square = 0\n",
        "        for i in range(len(point1)):\n",
        "\n",
        "            #compute the difference between each corresponding features\n",
        "            diff = (point1[i]-point2[i])\n",
        "            #square the difference to ensure distance is positive\n",
        "            squared = diff * diff\n",
        "\n",
        "            sum_of_square += squared\n",
        "\n",
        "        distance = sqrt(sum_of_square)\n",
        "        return distance\n",
        "\n",
        "    def get_k_nearest_neighbors(self, training_data, training_labels, new_point, k):\n",
        "        distances = []\n",
        "        for i in range (len(training_data)):\n",
        "            #get current example from the training dataset\n",
        "            current_point = training_data[i]\n",
        "\n",
        "            #calculate distance between new point and training point\n",
        "            dist = self.euclidean_distance(new_point, current_point)\n",
        "\n",
        "            #store pair\n",
        "            distances.append((dist, training_labels[i]))\n",
        "\n",
        "        #SORT DISTANCE\n",
        "        distances = sorted(distances, key=lambda x: x[0])\n",
        "        #select first k entries from sorted list\n",
        "        neighbours = [distances[i] for i in range(k)]\n",
        "\n",
        "        return neighbours\n",
        "\n",
        "    def majority_vote(self, neighbors):\n",
        "        label_count = dict()\n",
        "        for pair in neighbors:\n",
        "            label = pair[1]\n",
        "            if label not in label_count:\n",
        "                label_count[label] = 1\n",
        "            else:\n",
        "                label_count[label] += 1\n",
        "        # Find label with highest count\n",
        "\n",
        "        most_common_label = max(label_count, key=label_count.get)\n",
        "\n",
        "        return most_common_label\n",
        "\n",
        "    def predict(self, test_point):\n",
        "        distances = []\n",
        "\n",
        "        for sample, label in zip(self.X, self.y):\n",
        "            d = self.euclidean_distance(test_point, sample)\n",
        "            distances.append((d,label))\n",
        "\n",
        "        #sorting\n",
        "        distances = sorted(distances, key=lambda x: x[0])\n",
        "        #take the k nearest neighbour\n",
        "        k_neighbour = distances[:self.k]\n",
        "        #use majority vote to predict\n",
        "        predicted_label = self.majority_vote(k_neighbour)\n",
        "\n",
        "        return predicted_label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4adcaf9b"
      },
      "source": [
        "## Embedding Layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "66ba1a74"
      },
      "outputs": [],
      "source": [
        "class TokenEmbedding:\n",
        "    def __init__(self, vocab_size, embed_dim):\n",
        "        #create embedding matrix with random small values\n",
        "        self.W = random_matrix(shape=(vocab_size, embed_dim))\n",
        "\n",
        "    def forward(self, token_ids):\n",
        "        #token_ids : like [ 8,6,4,11]\n",
        "        embeddings = []\n",
        "        for id in token_ids:\n",
        "            #lookup = row from embedding matrix\n",
        "            vector = self.W[id]\n",
        "            embeddings.append(vector)\n",
        "        return embeddings\n",
        "\n",
        "class PosEmbedding:\n",
        "    def __init__(self,max_len, embed_dim):\n",
        "        self.P = random_matrix(shape=(max_len, embed_dim))\n",
        "    def forward(self, token_ids):\n",
        "        length = len(token_ids)\n",
        "        #add position bector to each token embedding\n",
        "        return [self.P[pos] for pos in range(length)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1943a58a"
      },
      "source": [
        "## SelfAttention Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0a1f7258"
      },
      "outputs": [],
      "source": [
        "'''Implement Q, K, V Weight Matrices + Compute Q, K, V vectors'''\n",
        "class SelfAttention:\n",
        "    def __init__(self, embed_dim):\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def forward(self, Q, K, V):\n",
        "        \"\"\"\n",
        "        Q, K, V shapes: (batch, seq_len, embed_dim)\n",
        "        Return:\n",
        "            output: (batch, seq_len, embed_dim)\n",
        "            weights: (batch, seq_len, seq_len)\n",
        "        \"\"\"\n",
        "\n",
        "        B, T, D = Q.shape\n",
        "\n",
        "        # 1. Compute attention scores = QÂ·K^T\n",
        "        # shape -> (B, T, T)\n",
        "        scores = np.matmul(Q, K.transpose(0, 2, 1))\n",
        "\n",
        "        # 2. Scale\n",
        "        scores = scores / np.sqrt(D)\n",
        "\n",
        "        # 3. Softmax across last dimension\n",
        "        exp_scores = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n",
        "        weights = exp_scores / np.sum(exp_scores, axis=-1, keepdims=True)\n",
        "\n",
        "        # 4. Weighted sum over V\n",
        "        # (B, T, T) @ (B, T, D) -> (B, T, D)\n",
        "        output = np.matmul(weights, V)\n",
        "\n",
        "        return output, weights\n",
        "\n",
        "    def dot(self,a,b):\n",
        "        sum = 0\n",
        "        for i in range(len(a)):\n",
        "            sum += a[i] * b[i]\n",
        "        return sum\n",
        "\n",
        "    def soft_max(self, scores):\n",
        "        scores = np.array(scores, dtype=np.float64)   # convert to array\n",
        "        scores = scores - np.max(scores)             # numerical stability\n",
        "        exps = np.exp(scores)                         # numpy exp works on arrays\n",
        "        probs = exps / np.sum(exps, axis=-1, keepdims=True)\n",
        "        return probs\n",
        "\n",
        "    def attention_scores(self, Q, K):\n",
        "        matrix = []\n",
        "        for qi in Q:\n",
        "            row = []\n",
        "            for kj in K:\n",
        "                row.append(self.dot(qi, kj))\n",
        "            matrix.append(row)\n",
        "        return matrix\n",
        "\n",
        "    def weighted_sum(self, weights, V):\n",
        "        dim = len(V[0])\n",
        "        result = [0.0 for _ in range(dim)]\n",
        "        for i in range(len(V)):\n",
        "            for d in range(dim):\n",
        "                result[d] += weights[i] * V[i][d]\n",
        "        return result\n",
        "\n",
        "    def compute_attention(self,Q,K,V):\n",
        "        scores = self.attention_scores(Q,K)\n",
        "        # scale\n",
        "        for i in range(len(scores)):\n",
        "            for j in range(len(scores[i])):\n",
        "                scores[i][j] /= sqrt(self.embed_dim)\n",
        "        output = []\n",
        "        for row in scores:\n",
        "            w = self.soft_max(row)\n",
        "            out_vec = self.weighted_sum(w, V)\n",
        "            output.append(out_vec)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5036297e"
      },
      "source": [
        "## AttentionHead Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "6e6759f4"
      },
      "outputs": [],
      "source": [
        "class AttentionHead:\n",
        "    def __init__(self, embed_dim, head_dim):\n",
        "        self.embed_dim = embed_dim\n",
        "        self.head_dim = head_dim\n",
        "\n",
        "        self.W_q = random_matrix((embed_dim, head_dim))\n",
        "        self.W_k = random_matrix((embed_dim, head_dim))\n",
        "        self.W_v = random_matrix((embed_dim, head_dim))\n",
        "\n",
        "    # -------------------------\n",
        "    # Copy these 5 methods below\n",
        "    # -------------------------\n",
        "    def dot(self, a, b):\n",
        "        s = 0\n",
        "        for i in range(len(a)):\n",
        "            s += a[i] * b[i]\n",
        "        return s\n",
        "\n",
        "    def attention_scores(self, Q, K):\n",
        "        matrix = []\n",
        "        for qi in Q:\n",
        "            row = []\n",
        "            for kj in K:\n",
        "                row.append(self.dot(qi, kj))\n",
        "            matrix.append(row)\n",
        "        return matrix\n",
        "\n",
        "    def soft_max(self, scores):\n",
        "        scores = np.array(scores, dtype=np.float64)   # convert to array\n",
        "        scores = scores - np.max(scores)             # numerical stability\n",
        "        exps = np.exp(scores)                         # numpy exp works on arrays\n",
        "        probs = exps / np.sum(exps, axis=-1, keepdims=True)\n",
        "        return probs\n",
        "\n",
        "    def weighted_sum(self, weights, V):\n",
        "        dim = len(V[0])\n",
        "        result = [0.0 for _ in range(dim)]\n",
        "\n",
        "        for i in range(len(V)):\n",
        "            for d in range(dim):\n",
        "                result[d] += weights[i] * V[i][d]\n",
        "        return result\n",
        "\n",
        "    def compute_attention(self, Q, K, V):\n",
        "        scores = self.attention_scores(Q, K)\n",
        "        for i in range(len(scores)):\n",
        "            for j in range(len(scores[i])):\n",
        "                scores[i][j] /= sqrt(self.head_dim)\n",
        "\n",
        "        output = []\n",
        "        for row in scores:\n",
        "            w = self.soft_max(row)\n",
        "            out_vec = self.weighted_sum(w, V)\n",
        "            output.append(out_vec)\n",
        "        return output\n",
        "\n",
        "    # -------------------------\n",
        "    # Main forward\n",
        "    # -------------------------\n",
        "    def forward(self, X):\n",
        "        Q, K, V = [], [], []\n",
        "        for token_vec in X:\n",
        "            Q.append(matmul(token_vec, self.W_q))\n",
        "            K.append(matmul(token_vec, self.W_k))\n",
        "            V.append(matmul(token_vec, self.W_v))\n",
        "\n",
        "        return self.compute_attention(Q, K, V)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8ea2580"
      },
      "source": [
        "## MultiHeadAttention Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1851d2f7"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention:\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        self.heads = [ AttentionHead(embed_dim, self.head_dim) for _ in range(num_heads) ]\n",
        "\n",
        "        # ðŸ”¥ Final linear layer after concatenation\n",
        "        self.Wo = random_matrix((embed_dim, embed_dim))\n",
        "\n",
        "    def concat_along_last_dim(self, head_outputs):\n",
        "        # head_outputs: list of [seq_len x head_dim] arrays\n",
        "        seq_len = len(head_outputs[0])\n",
        "        num_heads = len(head_outputs)\n",
        "        head_dim = len(head_outputs[0][0])\n",
        "\n",
        "        # initialize result\n",
        "        result = []\n",
        "\n",
        "        # loop over tokens\n",
        "        for i in range(seq_len):\n",
        "            concatenated = []\n",
        "            for head in head_outputs:\n",
        "                concatenated.extend(head[i])  # append head vector for token i\n",
        "            result.append(concatenated)\n",
        "\n",
        "        return result\n",
        "\n",
        "    def forward(self, X):\n",
        "        # X shape: [seq_len, embed_dim]\n",
        "\n",
        "        head_outputs = []\n",
        "\n",
        "        for head in self.heads:\n",
        "            # for each head, run attention\n",
        "            # BUT: X must first be projected down to head_dim\n",
        "\n",
        "            # Create Q,K,V using head weight matrices\n",
        "            # (selfattention already does this)\n",
        "\n",
        "            out = head.forward(X)\n",
        "            # shape: [seq_len, head_dim]\n",
        "\n",
        "            head_outputs.append(out)\n",
        "\n",
        "        # concatenate outputs from all heads\n",
        "        # final shape: [seq_len, embed_dim]\n",
        "        concatenated = self.concat_along_last_dim(head_outputs)\n",
        "\n",
        "        # final linear projection\n",
        "        final_output = []\n",
        "        for vec in concatenated:\n",
        "            final_output.append(matmul(vec, self.Wo))\n",
        "\n",
        "        return final_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8a4d632"
      },
      "source": [
        "## FeedForward and LayerNorm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2e18a03b"
      },
      "outputs": [],
      "source": [
        "def zeros(n):\n",
        "    return [0.0 for _ in range(n)]\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(x, 0)\n",
        "\n",
        "\n",
        "def add_vectors(a, b):\n",
        "    \"\"\"Element-wise add two vectors (same length).\"\"\"\n",
        "    return [a[i] + b[i] for i in range(len(a))]\n",
        "def add_vectors_list(A, B):\n",
        "    return [add_vectors(A[i], B[i]) for i in range(len(A))]\n",
        "class FeedForward:\n",
        "    def __init__(self, embed_dim, hidden_dim):\n",
        "        self.W1 = np.random.uniform(-0.1, 0.1, (embed_dim, hidden_dim))\n",
        "        self.b1 = np.zeros(hidden_dim)\n",
        "\n",
        "        self.W2 = np.random.uniform(-0.1, 0.1, (hidden_dim, embed_dim))\n",
        "        self.b2 = np.zeros(embed_dim)\n",
        "\n",
        "\n",
        "    def forward(self, X):\n",
        "        # X shape: (B, T, D)\n",
        "        hidden = np.matmul(X, self.W1) + self.b1\n",
        "        hidden = np.maximum(hidden, 0)   # ReLU\n",
        "        out = np.matmul(hidden, self.W2) + self.b2\n",
        "        return out\n",
        "\n",
        "\n",
        "class LayerNorm:\n",
        "    def __init__(self, dim, eps=1e-5):\n",
        "        self.dim = dim\n",
        "        self.eps = eps\n",
        "        # Learnable parameters (gamma, beta)\n",
        "        self.gamma = [1.0] * dim\n",
        "        self.beta = [0.0] * dim\n",
        "\n",
        "    def forward(self, X):\n",
        "        # X is list of vectors: [seq_len][dim]\n",
        "        out = []\n",
        "        for vec in X:\n",
        "            mean = sum(vec) / self.dim\n",
        "            var = sum((v - mean)**2 for v in vec) / self.dim\n",
        "            std = (var + self.eps) ** 0.5\n",
        "\n",
        "            norm = [ (vec[i] - mean) / std for i in range(len(vec)) ]\n",
        "            out.append([ norm[i] * self.gamma[i] + self.beta[i] for i in range(len(vec)) ])\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cf9bf4b"
      },
      "source": [
        "## TransformerBlock Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "6c562d5c"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock:\n",
        "    def __init__(self, embed_dim, num_heads, ff_hidden_dim):\n",
        "        self.ln1 = LayerNorm(embed_dim)\n",
        "        self.mha = MultiHeadAttention(embed_dim, num_heads)\n",
        "        self.ln2 = LayerNorm(embed_dim)\n",
        "        self.ff = FeedForward(embed_dim, ff_hidden_dim)\n",
        "\n",
        "    def forward(self, X):\n",
        "        # if batch dim exists, drop it for simplicity\n",
        "        if isinstance(X, np.ndarray) and X.ndim == 3:\n",
        "            X = X[0].tolist()  # shape -> [seq_len, embed_dim]\n",
        "\n",
        "        # LayerNorm + MultiHeadAttention\n",
        "        normed = self.ln1.forward(X)\n",
        "        attn_out = self.mha.forward(normed)  # shape: [seq_len, embed_dim]\n",
        "\n",
        "        # Residual\n",
        "        x2 = add_vectors_list(X, attn_out)\n",
        "\n",
        "        # LayerNorm + FeedForward\n",
        "        normed2 = self.ln2.forward(x2)\n",
        "        ff_out = self.ff.forward(normed2)\n",
        "\n",
        "        # Residual\n",
        "        out = add_vectors_list(x2, ff_out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac89fc71"
      },
      "source": [
        "## TransformerModel Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "840448c1"
      },
      "outputs": [],
      "source": [
        "class TransformerModel:\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, ffn_hidden_dim, max_len=128):\n",
        "        self.token_embed = TokenEmbedding(vocab_size, embed_dim)\n",
        "        self.pos_embed = PosEmbedding(max_len, embed_dim)\n",
        "        self.blocks = [TransformerBlock(embed_dim, num_heads, ffn_hidden_dim)\n",
        "                       for _ in range(num_layers)]\n",
        "        self.Wo = random_matrix((embed_dim, vocab_size))\n",
        "        self.bo = np.zeros(vocab_size)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.max_len = max_len\n",
        "\n",
        "\n",
        "    def forward(self, ids):\n",
        "        # ids: list of token IDs\n",
        "        tok_vecs = self.token_embed.forward(ids)\n",
        "        pos_vecs = self.pos_embed.forward(ids)\n",
        "        X = [add_vectors(tok_vecs[i], pos_vecs[i]) for i in range(len(ids))]  # seq_len x embed_dim\n",
        "\n",
        "        # pass through blocks\n",
        "        for block in self.blocks:\n",
        "            X = block.forward(X)  # output: seq_len x embed_dim\n",
        "\n",
        "        # final linear layer\n",
        "        logits = np.matmul(X, self.Wo) + self.bo  # seq_len x vocab_size\n",
        "        return logits\n",
        "\n",
        "    def generate(self, idx, max_new_tokens, tokenizer):\n",
        "        \"\"\"\n",
        "        idx: list of token ids (context)\n",
        "        max_new_tokens: how many tokens to generate\n",
        "        tokenizer: your BPE or simple tokenizer\n",
        "        \"\"\"\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            # forward pass: logits shape (T, vocab)\n",
        "            logits = self.forward(idx)\n",
        "\n",
        "            # take last position\n",
        "            last_logits = logits[-1]   # shape: (vocab,)\n",
        "\n",
        "            # convert to probabilities using softmax\n",
        "            exps = np.exp(last_logits - np.max(last_logits))\n",
        "            probs = exps / np.sum(exps)\n",
        "\n",
        "            # sample from distribution (probabilistic)\n",
        "            next_id = int(np.random.choice(len(probs), p=probs))\n",
        "\n",
        "\n",
        "            # append prediction\n",
        "            idx.append(next_id)\n",
        "\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c280e69"
      },
      "source": [
        "## MiniTransformer Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "abf4885c"
      },
      "outputs": [],
      "source": [
        "class MiniTransformer:\n",
        "    def __init__(self, vocab_size, max_len, embed_dim, num_heads, ff_hidden_dim, num_layers):\n",
        "        self.token_embed = TokenEmbedding(vocab_size, embed_dim)\n",
        "        self.pos_embed = PosEmbedding(max_len, embed_dim)\n",
        "\n",
        "        self.blocks = [\n",
        "            TransformerBlock(embed_dim, num_heads, ff_hidden_dim)\n",
        "            for _ in range(num_layers)\n",
        "        ]\n",
        "\n",
        "    def forward(self, token_ids):\n",
        "        tok = self.token_embed.forward(token_ids)\n",
        "        pos = self.pos_embed.forward(token_ids)\n",
        "\n",
        "        X = add_vectors_list(tok, pos)\n",
        "\n",
        "        for block in self.blocks:\n",
        "            X = block.forward(X)\n",
        "\n",
        "        return X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "782e94c4"
      },
      "source": [
        "## Example Usage: Tokenizer Training and Model Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ea64aa00"
      },
      "outputs": [],
      "source": [
        "# Example tiny corpus\n",
        "text = \"hello world hello transformer model mini gpt\"\n",
        "\n",
        "# Initialize and train tokenizer\n",
        "tokenizer = BPETokenizer(vocab_size=50)\n",
        "tokenizer.train(text)\n",
        "\n",
        "# Convert text to token IDs\n",
        "token_ids = tokenizer.encode(text)  # e.g., [1, 5, 1, 20, 3, ...]\n",
        "seq_len = 4  # number of tokens in input\n",
        "X_train = []\n",
        "y_train = []\n",
        "\n",
        "for i in range(len(token_ids) - seq_len):\n",
        "    X_train.append(token_ids[i:i+seq_len])\n",
        "    y_train.append(token_ids[i+seq_len])\n",
        "\n",
        "X_train = np.array(X_train)  # shape: (num_samples, seq_len)\n",
        "y_train = np.array(y_train)  # shape: (num_samples,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "952cd46a"
      },
      "source": [
        "## Model Initialization and Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf39f6e5",
        "outputId": "2b0508b2-cfbe-43ab-b030-3d9e25ebcece"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, loss: 1.1181\n",
            "Epoch 10, loss: 0.8536\n",
            "Epoch 20, loss: 0.6367\n",
            "Epoch 30, loss: 0.4850\n",
            "Epoch 40, loss: 0.3918\n",
            "Epoch 50, loss: 0.3369\n",
            "Epoch 60, loss: 0.3035\n",
            "Epoch 70, loss: 0.2821\n",
            "Epoch 80, loss: 0.2676\n",
            "Epoch 90, loss: 0.2572\n",
            "Epoch 100, loss: 0.2495\n",
            "Epoch 110, loss: 0.2435\n",
            "Epoch 120, loss: 0.2389\n",
            "Epoch 130, loss: 0.2351\n",
            "Epoch 140, loss: 0.2319\n",
            "Epoch 150, loss: 0.2293\n",
            "Epoch 160, loss: 0.2270\n",
            "Epoch 170, loss: 0.2251\n",
            "Epoch 180, loss: 0.2234\n",
            "Epoch 190, loss: 0.2219\n"
          ]
        }
      ],
      "source": [
        "lr = 0.01  # learning rate\n",
        "model = TransformerModel(vocab_size=tokenizer.vocab_size, embed_dim=16,\n",
        "                         num_heads=2, num_layers=2, ffn_hidden_dim=32)\n",
        "\n",
        "epochs = 100\n",
        "import numpy as np\n",
        "\n",
        "# --- Hyperparameters ---\n",
        "lr = 0.1        # learning rate\n",
        "epochs = 200\n",
        "seq_len = 5     # small sequence length for tiny corpus\n",
        "\n",
        "# Assume you already have:\n",
        "# model: TransformerModel instance\n",
        "# tokenizer: BPETokenizer instance\n",
        "# text: training text (string)\n",
        "\n",
        "# Encode the text into token IDs\n",
        "ids = tokenizer.encode(text)\n",
        "\n",
        "# Training loop (simple next-token prediction)\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0.0\n",
        "    for i in range(len(ids) - seq_len):\n",
        "        x_ids = ids[i:i+seq_len]\n",
        "        y_id = ids[i+seq_len]\n",
        "\n",
        "        logits = model.forward(x_ids)\n",
        "        pred = logits[-1]  # last token logits\n",
        "\n",
        "        # softmax & loss\n",
        "        exp_pred = np.exp(pred - np.max(pred))\n",
        "        probs = exp_pred / np.sum(exp_pred)\n",
        "        loss = -np.log(probs[y_id] + 1e-8)\n",
        "        total_loss += loss\n",
        "\n",
        "        # gradient for Wo & bo\n",
        "        grad = probs.copy()\n",
        "        grad[y_id] -= 1.0\n",
        "\n",
        "        # last hidden vector\n",
        "        X_block = model.blocks[-1].forward(\n",
        "            [add_vectors(model.token_embed.forward(x_ids)[-1],\n",
        "                         model.pos_embed.forward(x_ids)[-1])]\n",
        "        )\n",
        "        h = np.array(X_block[-1])\n",
        "\n",
        "        # update\n",
        "        model.Wo -= lr * np.outer(h, grad)\n",
        "        model.bo -= lr * grad\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch}, loss: {total_loss / len(ids):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca5a984b"
      },
      "source": [
        "## Text Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79566418",
        "outputId": "0c2bd399-27b5-4332-bb55-66524be7873d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hello world gpt mini mini mini mini mini mini mini gpt gpt gpt mini gpt gpt gpt gpt mini gpt mini mini\n"
          ]
        }
      ],
      "source": [
        "prompt = \"hello world\"\n",
        "ids = tokenizer.encode(prompt)\n",
        "generated_ids = model.generate(ids, max_new_tokens=20, tokenizer=tokenizer)\n",
        "print(tokenizer.decode(generated_ids))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f45b1498"
      },
      "source": [
        "## Test Cases (Commented Out in Original Code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8c993645",
        "outputId": "b574889e-430e-472e-fc1c-dad54bf712e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A\n",
            "B\n",
            "{'<unk>': 0, '<pad>': 1, ' ': 2, 't': 3, 'h': 4, 'i': 5, 's': 6, 'a': 7, 'e': 8, '.': 9, 'f': 10, 'u': 11, 'n': 12, 'is': 13, 'th': 14, 'this': 15, 'te': 16, 'tes': 17, 'test': 18, 'test.': 19, 'fu': 20, 'fun': 21, 'fun.': 22}\n",
            "[('i', 's'), ('t', 'h'), ('th', 'is'), ('t', 'e'), ('te', 's'), ('tes', 't'), ('test', '.'), ('f', 'u'), ('fu', 'n'), ('fun', '.')]\n",
            "[11, 9, 7, 14]\n",
            "[[0.005435090445325505, 0.005214070070302522, 0.0006359025062336876, -0.002854801812955419, -0.004275208388000406, 0.013866135981498919, -0.0029498332172676013, -0.007478222692720853, 0.015692949050604598, 0.007469832578322387, 0.01453455113080112, 0.007797396504168944, 0.004605358125144905, -0.003331052100171203, 0.0010340156020282902, -0.013234496141904514, 0.002529917203190255, 0.008919343915569812, 0.008782733529558114, 0.0005754059831637251, -0.0013609005211425475, -0.001440862954472202, -0.0013495129033858286, -0.013797365522932904, -0.005525580194372901, 0.003583265850876984, -0.005000024153293623, -0.005806137261935515, -0.011012699066456017, 0.015597235407083458, -0.009292224954580803, -0.007105612535406227], [0.002335878308564507, -0.011300951223585442, -0.0007481399548812315, -0.00483264816090809, -0.007295195815562674, -0.013783728734929269, -0.007822100035992974, -0.007435968853225552, 0.010388592280872565, -0.004007117651892887, 0.00358963106516272, 0.007379034693039573, -0.003830890163030164, 0.007556339625084448, 0.003597695388517794, -0.016413227525793765, 0.0028573203724243986, 0.004907560881919648, 0.007847880902676118, 0.010643378464080703, -0.0012247891542743576, 0.003138679521575316, -0.009775341676014033, -0.004575227439666256, 0.011545472611126813, 0.008876289057425726, -0.013984191014407642, -0.003131483389557394, -0.003923515281981496, -0.00564018374252245, -0.015478766840384852, -0.004082339741447337], [0.0035997486105674714, 0.00416752799266204, 0.005619331799818874, -0.006121226757162417, -0.001682369129627586, 0.007021587743390777, -0.006397258318244059, -0.0005628072853232843, 0.013508604068382125, 0.01755114012023871, -0.010955251222930642, -0.012111896021280506, 0.009715629809841725, 0.0018883300091779437, 0.0017813941163751148, 0.0060619601184936055, -0.005156873038301035, 0.010863052241456537, -0.0022688908360281902, -0.014206307059670496, 0.008849126312363021, 1.6321664446018735e-05, -0.004001563307088741, -0.009690144245741904, -0.0015634841668548775, -0.008421918149696887, -0.00045650723251513224, -0.017924487660064842, 0.005358797225879182, 0.012169902227466257, -0.003136517139445942, 0.0011644389108674685], [0.011034166258659822, 0.009321617654269912, -0.006916008174206294, -0.008712257201950416, -0.0011306046105369226, -0.006617225925107738, -0.0048240668845551515, -0.0005424983132074623, 0.008273489624466761, 0.006797790524629692, 0.012567508210676735, 0.0038795103099842822, 0.001322354579181258, -0.019063820970847103, -0.0035820578472171944, -0.0017322626250225216, -0.0042294811682957855, -0.016379578853980274, 0.0013446283257666945, -0.008756516092098519, -7.62620961061028e-05, -0.008028632117863334, -0.004102727796920436, 0.014936390412478403, 0.003204070701664976, 0.0059785394192902915, -0.007914544793663767, -0.013306420432017022, -0.011069732251445814, -0.002851484359474527, -0.00522848549253173, -0.010813711372283121]]\n",
            "Output shape: (1, 2, 4)\n",
            "Weights shape: (1, 2, 2)\n",
            "block out shape: 4 4\n",
            "[[np.float64(0.025629402756950376), np.float64(6.603397535282982e-05), np.float64(-0.02952954569237691), np.float64(-0.012072984510941178)], [np.float64(0.025767407333122976), np.float64(0.0162197645394973), np.float64(-0.008617419936093279), np.float64(0.045215989572901336)], [np.float64(0.0058194872013533055), np.float64(0.011435798702592155), np.float64(-0.011529358812369904), np.float64(-0.007261628533134971)], [np.float64(0.050691206650356785), np.float64(-0.00992135446450282), np.float64(-0.03902284896262223), np.float64(0.004027221697727384)]]\n",
            "block out shape: 4 4\n",
            "[[np.float64(0.025629402756950376), np.float64(6.603397535282982e-05), np.float64(-0.02952954569237691), np.float64(-0.012072984510941178)], [np.float64(0.025767407333122976), np.float64(0.0162197645394973), np.float64(-0.008617419936093279), np.float64(0.045215989572901336)], [np.float64(0.0058194872013533055), np.float64(0.011435798702592155), np.float64(-0.011529358812369904), np.float64(-0.007261628533134971)], [np.float64(0.050691206650356785), np.float64(-0.00992135446450282), np.float64(-0.03902284896262223), np.float64(0.004027221697727384)]]\n",
            "Testing full TransformerBlock...\n",
            "block out shape: 4 4\n",
            "[[np.float64(0.0084286714835691), np.float64(-0.07238335024932363), np.float64(0.03522446587297487), np.float64(-0.024122970587170264)], [np.float64(-7.955320492416575e-05), np.float64(-0.013819203208813743), np.float64(-0.008504320135555849), np.float64(-0.0006176555601906247)], [np.float64(0.0030571818672799535), np.float64(-0.0017103518478316735), np.float64(0.010329557326572971), np.float64(-0.004948147521023581)], [np.float64(0.017615519807212404), np.float64(-0.0911145302928595), np.float64(0.03676571208322711), np.float64(-0.015675732603560276)]]\n",
            "mha out shape: 4 4\n",
            "logits shape: 5 200\n",
            "hello t i bui there yo h building a <pad> bui re model a are n mode tra transf ho langua\n"
          ]
        }
      ],
      "source": [
        "'''Test cases for all the clasees and functions'''\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    X_knn = [\n",
        "        [1, 2],\n",
        "        [2, 3],\n",
        "        [3, 3],\n",
        "        [8, 7],\n",
        "        [9, 8],\n",
        "        [10, 8]\n",
        "    ]\n",
        "\n",
        "    y = [\"A\", \"A\", \"A\", \"B\", \"B\", \"B\"]\n",
        "\n",
        "    knn = KNN(k=3)\n",
        "    knn.fit(X_knn, y)\n",
        "\n",
        "    print(knn.predict([2, 2]))   # should give \"A\"\n",
        "    print(knn.predict([9, 7]))   # should give \"B\"\n",
        "\n",
        "\n",
        "    text = \"this is a test. this test is fun.\"\n",
        "    tokenizer = BPETokenizer(vocab_size=50)\n",
        "    tokenizer.train(text)\n",
        "    print(tokenizer.vocab)\n",
        "    print(tokenizer.merges)\n",
        "\n",
        "    text = \"this is a test\"\n",
        "    tokenizer = BPETokenizer(vocab_size=50)\n",
        "    tokenizer.train(text)\n",
        "\n",
        "    ids = tokenizer.encode(\"this is a test\")\n",
        "    print(ids)\n",
        "\n",
        "    decoded_text = tokenizer.decode(ids)\n",
        "    # print(decoded_text)\n",
        "\n",
        "\n",
        "\n",
        "    def add_vectors(a,b):\n",
        "        return [a[i] + b[i] for i in range(len(a))]\n",
        "\n",
        "    token_embed = TokenEmbedding(vocab_size=1000, embed_dim=32)\n",
        "    pos_embed = PosEmbedding(max_len=512, embed_dim=32)\n",
        "\n",
        "    ids = [8,6,4,11]\n",
        "\n",
        "    token_vectors = token_embed.forward(ids)\n",
        "    pos_vectors   = pos_embed.forward(ids)\n",
        "\n",
        "    final_vectors = [\n",
        "        add_vectors(token_vectors[i], pos_vectors[i])\n",
        "        for i in range(len(ids))\n",
        "    ]\n",
        "\n",
        "    print(final_vectors)\n",
        "\n",
        "    #Fake tiny example for SelfAttention\n",
        "    embed_dim = 4\n",
        "    X_attention_input = [\n",
        "        [0.1, 0.2, 0.3, 0.4],\n",
        "        [0.5, 0.4, 0.3, 0.2]\n",
        "    ]\n",
        "\n",
        "    att = SelfAttention(embed_dim)\n",
        "\n",
        "    # Convert X_attention_input to a numpy array with a batch dimension\n",
        "    # Expected shape for Q, K, V in SelfAttention.forward is (batch, seq_len, embed_dim)\n",
        "    X_np_for_attention = np.array(X_attention_input).reshape(1, len(X_attention_input), embed_dim)\n",
        "\n",
        "    # Pass X_np_for_attention for Q, K, and V as a simple test for self-attention\n",
        "    output_att, weights_att = att.forward(X_np_for_attention, X_np_for_attention, X_np_for_attention)\n",
        "\n",
        "    print(\"Output shape:\", output_att.shape)\n",
        "    print(\"Weights shape:\", weights_att.shape)\n",
        "\n",
        "    # ---- BUILD INPUT X ----\n",
        "    tokenizer = BPETokenizer(vocab_size=1000)\n",
        "    tokenizer.train(\"this is a test corpus for building tiny gpt tokenizer\")\n",
        "    token_embedding = TokenEmbedding(vocab_size=1000, embed_dim=4)\n",
        "    pos_embedding   = PosEmbedding(max_len=50, embed_dim=4)\n",
        "\n",
        "    ids = tokenizer.encode(\"this is a test\")\n",
        "\n",
        "    token_embed = token_embedding.forward(ids)\n",
        "    pos_embed = pos_embedding.forward(ids)\n",
        "\n",
        "    # Add token+pos embeddings\n",
        "    X_transformer_block = []\n",
        "    for i in range(len(token_embed)):\n",
        "        vec = []\n",
        "        for a, b in zip(token_embed[i], pos_embed[i]):\n",
        "            vec.append(a + b)\n",
        "        X_transformer_block.append(vec)\n",
        "\n",
        "    # ---- RUN TRANSFORMER BLOCK ----\n",
        "    tb = TransformerBlock(embed_dim=4, num_heads=2, ff_hidden_dim=16)\n",
        "    out = tb.forward(X_transformer_block)\n",
        "\n",
        "    print(\"block out shape:\", len(out), len(out[0]))\n",
        "    print(out)\n",
        "\n",
        "    print(\"block out shape:\", len(out), len(out[0]))\n",
        "    print(out)\n",
        "\n",
        "\n",
        "    print(\"Testing full TransformerBlock...\")\n",
        "    tb = TransformerBlock(embed_dim=4, num_heads=2, ff_hidden_dim=16)\n",
        "\n",
        "    out = tb.forward(X_transformer_block)\n",
        "    print(\"block out shape:\", len(out), len(out[0]))\n",
        "    print(out)\n",
        "\n",
        "    print(\"mha out shape:\", len(out), len(out[0]))  # expect seq_len x embed_dim\n",
        "\n",
        "    # small test X (seq_len=4, embed_dim must match your token/embed dims)\n",
        "\n",
        "\n",
        "    embed_dim = 4\n",
        "    num_heads = 2  # head_dim = 2\n",
        "    token_embedding = TokenEmbedding(vocab_size=100, embed_dim=embed_dim)\n",
        "    pos_embedding = PosEmbedding(max_len=20, embed_dim=embed_dim)\n",
        "    tokenizer = BPETokenizer(vocab_size=100)\n",
        "    tokenizer.train(\"this is a test\")   # small corpus ok\n",
        "\n",
        "    ids = tokenizer.encode(\"this is a test\")\n",
        "    token_vectors = token_embedding.forward(ids)\n",
        "    pos_vectors   = pos_embedding.forward(ids)\n",
        "    X_mha = [[a+b for a,b in zip(token_vectors[i], pos_vectors[i])] for i in range(len(ids))]\n",
        "\n",
        "    mha = MultiHeadAttention(embed_dim=embed_dim, num_heads=num_heads)\n",
        "    out = mha.forward(X_mha)\n",
        "    tb = TransformerBlock(embed_dim=4, num_heads=2, ff_hidden_dim=16)\n",
        "    out = tb.forward(X_mha)\n",
        "\n",
        "    model = TransformerModel(vocab_size=200, max_len=32, embed_dim=8, num_heads=2, num_layers=2, ffn_hidden_dim=32)\n",
        "    tokenizer = BPETokenizer(vocab_size=200)\n",
        "    tokenizer.train(\"this is a tiny corpus for testing\")\n",
        "    ids = tokenizer.encode(\"this is a test\")\n",
        "    logits = model.forward(ids)\n",
        "    print(\"logits shape:\", len(logits), len(logits[0]))   # expect seq_len x vocab_size\n",
        "\n",
        "    training_text = \"\"\"\n",
        "    hello world this is a tiny training dataset\n",
        "    hello there how are you\n",
        "    i am building a tiny transformer language model\n",
        "    \"\"\"\n",
        "\n",
        "    tokenizer = BPETokenizer(vocab_size=1000)\n",
        "    tokenizer.train(training_text)\n",
        "\n",
        "    # ACTUAL vocab size after training\n",
        "    vocab_size = len(tokenizer.vocab)\n",
        "\n",
        "    model = TransformerModel(vocab_size, embed_dim=64, num_heads=4, num_layers=2, ffn_hidden_dim=128)\n",
        "\n",
        "    prompt = \"hello\"\n",
        "    ids = tokenizer.encode(prompt)\n",
        "\n",
        "    generated = model.generate(ids, max_new_tokens=20, tokenizer=tokenizer)\n",
        "\n",
        "    print(tokenizer.decode(generated))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPL3rQLi20NCOthJkzlR6pO",
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
