{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPL3rQLi20NCOthJkzlR6pO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Suraj-Sedai/Transformer-language-model-MiniGPT/blob/main/transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaa00d69"
      },
      "source": [
        "# Mini Transformer Model Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73e40c6b"
      },
      "source": [
        "## Imports and Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5b61987"
      },
      "source": [
        "from math import sqrt,exp\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "'''Some helper functions'''\n",
        "\n",
        "def random_matrix(shape):\n",
        "    rows, cols = shape\n",
        "    return [\n",
        "        [random.uniform(-0.01, 0.01) for _ in range(cols)]\n",
        "        for _ in range(rows)\n",
        "    ]\n",
        "def matmul(vec, mat):\n",
        "    # vec: [m]\n",
        "    # mat: [m][n]\n",
        "    m = len(vec)\n",
        "    n = len(mat[0])\n",
        "    result = [0]*n\n",
        "\n",
        "    for col in range(n):\n",
        "        s = 0\n",
        "        for row in range(m):\n",
        "            s += vec[row] * mat[row][col]\n",
        "        result[col] = s\n",
        "\n",
        "    return result\n",
        "\n",
        "# v = [1,2,3]\n",
        "# m = [\n",
        "#     [1,0],\n",
        "#     [0,1],\n",
        "#     [1,1]\n",
        "# ]\n",
        "# print(matmul(v, m))\n",
        "\n",
        "def layer_norm(X):\n",
        "    # X = [seq_len, embed_dim]\n",
        "    output = []\n",
        "    for token_vec in X:\n",
        "        mean = sum(token_vec)/len(token_vec)\n",
        "        variance = sum((v-mean)**2 for v in token_vec)/len(token_vec)\n",
        "        std = sqrt(variance + 1e-5)\n",
        "        normalized = [(v-mean)/std for v in token_vec]\n",
        "        output.append(normalized)\n",
        "    return output\n",
        "\n",
        "def softmax(x):\n",
        "    e = np.exp(x - np.max(x))  # for numerical stability\n",
        "    return e / e.sum(axis=-1, keepdims=True)\n",
        "\n",
        "def cross_entropy_loss(logits, target_id):\n",
        "    probs = softmax(logits)\n",
        "    return -np.log(probs[target_id] + 1e-9)  # avoid log(0)\n",
        "\n",
        "def grad_cross_entropy(logits, target_id):\n",
        "    probs = softmax(logits)\n",
        "    probs[target_id] -= 1\n",
        "    return probs  # gradient w.r.t logits"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e3b75d7"
      },
      "source": [
        "## BPETokenizer Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ed2324d0"
      },
      "source": [
        "class BPETokenizer:\n",
        "    def __init__(self, vocab_size=1000):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.vocab = {}      # token -> id\n",
        "        self.inv_vocab = {}  # id -> token\n",
        "        self.merges = []     # list of merge rules\n",
        "\n",
        "    def train(self, text):\n",
        "        text = text.lower()\n",
        "\n",
        "        # ---- special tokens ----\n",
        "        for special in [\"<unk>\", \"<pad>\", \" \"]:\n",
        "            if special not in self.vocab:\n",
        "                self.vocab[special] = len(self.vocab)\n",
        "\n",
        "        # ---- split into words ----\n",
        "        words = text.strip().split()\n",
        "        tokens_list = [self._word_to_chars(w) for w in words]\n",
        "\n",
        "        # ---- add unique chars ----\n",
        "        for token_list in tokens_list:\n",
        "            for t in token_list:\n",
        "                if t not in self.vocab:\n",
        "                    self.vocab[t] = len(self.vocab)\n",
        "\n",
        "        # ---- BPE loop ----\n",
        "        while len(self.vocab) < self.vocab_size:\n",
        "            pair_counts = self.get_pair_frequencies(tokens_list)\n",
        "            if not pair_counts:\n",
        "                break\n",
        "\n",
        "            best_pair = max(pair_counts, key=pair_counts.get)\n",
        "            tokens_list = self._merge_pair(tokens_list, best_pair)\n",
        "\n",
        "            new_token = best_pair[0] + best_pair[1]\n",
        "            if new_token not in self.vocab:\n",
        "                self.vocab[new_token] = len(self.vocab)\n",
        "\n",
        "            self.merges.append(best_pair)\n",
        "\n",
        "        # ---- inverse vocab ----\n",
        "        self.inv_vocab = {idx: tok for tok, idx in self.vocab.items()}\n",
        "\n",
        "\n",
        "    def _word_to_chars(self, word):\n",
        "        # turn a word into a list of char\n",
        "        return list(word)\n",
        "\n",
        "    def encode(self, text):\n",
        "        text = text.lower().strip()\n",
        "        words = text.split()\n",
        "\n",
        "        # convert each word into list of characters\n",
        "        tokens_list = [self._word_to_chars(w) for w in words]\n",
        "\n",
        "        # apply BPE merges\n",
        "        for merge_pair in self.merges:\n",
        "            tokens_list = self._merge_pair(tokens_list, merge_pair)\n",
        "\n",
        "        # convert tokens into ids (safe lookup)\n",
        "        token_ids = []\n",
        "        for token_list in tokens_list:\n",
        "            for token in token_list:\n",
        "                token_ids.append(self.vocab.get(token, self.vocab[\"<unk>\"]))\n",
        "\n",
        "        return token_ids\n",
        "\n",
        "\n",
        "    def decode(self, token_ids):\n",
        "        tokens = [self.inv_vocab.get(i, \"<unk>\") for i in token_ids]\n",
        "        text = ' '.join(tokens)\n",
        "        return text\n",
        "\n",
        "\n",
        "    def get_pair_frequencies(self, tokens_list):\n",
        "        # get frequencies of adjacent token pairs\n",
        "        pair_counts = dict()\n",
        "        for token_list in tokens_list:\n",
        "            for i in range(len(token_list)-1):\n",
        "                pair = (token_list[i], token_list[i+1])\n",
        "\n",
        "                if pair not in pair_counts:\n",
        "                    pair_counts[pair] = 1\n",
        "                else:\n",
        "                    pair_counts[pair] +=1\n",
        "        return pair_counts\n",
        "\n",
        "    def _merge_pair(self, tokens_list,pair_to_merge):\n",
        "        #pair to merge in tuple\n",
        "        a = pair_to_merge[0]\n",
        "        b = pair_to_merge[1]\n",
        "        new_tokens_list = []\n",
        "        #processing each word one by one\n",
        "        for token_list in tokens_list:\n",
        "            merged_word = []\n",
        "            i = 0\n",
        "            while i < len(token_list):\n",
        "                if i < len(token_list)-1 and token_list[i] == a and token_list[i + 1] == b:\n",
        "                    #merge two token\n",
        "                    merged_token = a+b\n",
        "                    merged_word.append(merged_token)\n",
        "                    i +=2\n",
        "                else:\n",
        "                    merged_word.append(token_list[i])\n",
        "                    i +=1\n",
        "            #add processed word back to list\n",
        "            new_tokens_list.append(merged_word)\n",
        "        return new_tokens_list"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56c0dbb5"
      },
      "source": [
        "## KNN Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5f7d44c4"
      },
      "source": [
        "class KNN:\n",
        "    def __init__(self, k=3):\n",
        "        self.k = k\n",
        "        self.X = []\n",
        "        self.y = []\n",
        "\n",
        "    def fit(self,X,y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def euclidean_distance(self,point1, point2):\n",
        "        #point1 and point2\n",
        "        sum_of_square = 0\n",
        "        for i in range(len(point1)):\n",
        "\n",
        "            #compute the difference between each corresponding features\n",
        "            diff = (point1[i]-point2[i])\n",
        "            #square the difference to ensure distance is positive\n",
        "            squared = diff * diff\n",
        "\n",
        "            sum_of_square += squared\n",
        "\n",
        "        distance = sqrt(sum_of_square)\n",
        "        return distance\n",
        "\n",
        "    def get_k_nearest_neighbors(self, training_data, training_labels, new_point, k):\n",
        "        distances = []\n",
        "        for i in range (len(training_data)):\n",
        "            #get current example from the training dataset\n",
        "            current_point = training_data[i]\n",
        "\n",
        "            #calculate distance between new point and training point\n",
        "            dist = self.euclidean_distance(new_point, current_point)\n",
        "\n",
        "            #store pair\n",
        "            distances.append((dist, training_labels[i]))\n",
        "\n",
        "        #SORT DISTANCE\n",
        "        distances = sorted(distances, key=lambda x: x[0])\n",
        "        #select first k entries from sorted list\n",
        "        neighbours = [distances[i] for i in range(k)]\n",
        "\n",
        "        return neighbours\n",
        "\n",
        "    def majority_vote(self, neighbors):\n",
        "        label_count = dict()\n",
        "        for pair in neighbors:\n",
        "            label = pair[1]\n",
        "            if label not in label_count:\n",
        "                label_count[label] = 1\n",
        "            else:\n",
        "                label_count[label] += 1\n",
        "        # Find label with highest count\n",
        "\n",
        "        most_common_label = max(label_count, key=label_count.get)\n",
        "\n",
        "        return most_common_label\n",
        "\n",
        "    def predict(self, test_point):\n",
        "        distances = []\n",
        "\n",
        "        for sample, label in zip(self.X, self.y):\n",
        "            d = self.euclidean_distance(test_point, sample)\n",
        "            distances.append((d,label))\n",
        "\n",
        "        #sorting\n",
        "        distances = sorted(distances, key=lambda x: x[0])\n",
        "        #take the k nearest neighbour\n",
        "        k_neighbour = distances[:self.k]\n",
        "        #use majority vote to predict\n",
        "        predicted_label = self.majority_vote(k_neighbour)\n",
        "\n",
        "        return predicted_label"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4adcaf9b"
      },
      "source": [
        "## Embedding Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66ba1a74"
      },
      "source": [
        "class TokenEmbedding:\n",
        "    def __init__(self, vocab_size, embed_dim):\n",
        "        #create embedding matrix with random small values\n",
        "        self.W = random_matrix(shape=(vocab_size, embed_dim))\n",
        "\n",
        "    def forward(self, token_ids):\n",
        "        #token_ids : like [ 8,6,4,11]\n",
        "        embeddings = []\n",
        "        for id in token_ids:\n",
        "            #lookup = row from embedding matrix\n",
        "            vector = self.W[id]\n",
        "            embeddings.append(vector)\n",
        "        return embeddings\n",
        "\n",
        "class PosEmbedding:\n",
        "    def __init__(self,max_len, embed_dim):\n",
        "        self.P = random_matrix(shape=(max_len, embed_dim))\n",
        "    def forward(self, token_ids):\n",
        "        length = len(token_ids)\n",
        "        #add position bector to each token embedding\n",
        "        return [self.P[pos] for pos in range(length)]"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1943a58a"
      },
      "source": [
        "## SelfAttention Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0a1f7258"
      },
      "source": [
        "'''Implement Q, K, V Weight Matrices + Compute Q, K, V vectors'''\n",
        "class SelfAttention:\n",
        "    def __init__(self, embed_dim):\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def forward(self, Q, K, V):\n",
        "        \"\"\"\n",
        "        Q, K, V shapes: (batch, seq_len, embed_dim)\n",
        "        Return:\n",
        "            output: (batch, seq_len, embed_dim)\n",
        "            weights: (batch, seq_len, seq_len)\n",
        "        \"\"\"\n",
        "\n",
        "        B, T, D = Q.shape\n",
        "\n",
        "        # 1. Compute attention scores = QÂ·K^T\n",
        "        # shape -> (B, T, T)\n",
        "        scores = np.matmul(Q, K.transpose(0, 2, 1))\n",
        "\n",
        "        # 2. Scale\n",
        "        scores = scores / np.sqrt(D)\n",
        "\n",
        "        # 3. Softmax across last dimension\n",
        "        exp_scores = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n",
        "        weights = exp_scores / np.sum(exp_scores, axis=-1, keepdims=True)\n",
        "\n",
        "        # 4. Weighted sum over V\n",
        "        # (B, T, T) @ (B, T, D) -> (B, T, D)\n",
        "        output = np.matmul(weights, V)\n",
        "\n",
        "        return output, weights\n",
        "\n",
        "    def dot(self,a,b):\n",
        "        sum = 0\n",
        "        for i in range(len(a)):\n",
        "            sum += a[i] * b[i]\n",
        "        return sum\n",
        "\n",
        "    def soft_max(self, scores):\n",
        "        scores = np.array(scores, dtype=np.float64)   # convert to array\n",
        "        scores = scores - np.max(scores)             # numerical stability\n",
        "        exps = np.exp(scores)                         # numpy exp works on arrays\n",
        "        probs = exps / np.sum(exps, axis=-1, keepdims=True)\n",
        "        return probs\n",
        "\n",
        "    def attention_scores(self, Q, K):\n",
        "        matrix = []\n",
        "        for qi in Q:\n",
        "            row = []\n",
        "            for kj in K:\n",
        "                row.append(self.dot(qi, kj))\n",
        "            matrix.append(row)\n",
        "        return matrix\n",
        "\n",
        "    def weighted_sum(self, weights, V):\n",
        "        dim = len(V[0])\n",
        "        result = [0.0 for _ in range(dim)]\n",
        "        for i in range(len(V)):\n",
        "            for d in range(dim):\n",
        "                result[d] += weights[i] * V[i][d]\n",
        "        return result\n",
        "\n",
        "    def compute_attention(self,Q,K,V):\n",
        "        scores = self.attention_scores(Q,K)\n",
        "        # scale\n",
        "        for i in range(len(scores)):\n",
        "            for j in range(len(scores[i])):\n",
        "                scores[i][j] /= sqrt(self.embed_dim)\n",
        "        output = []\n",
        "        for row in scores:\n",
        "            w = self.soft_max(row)\n",
        "            out_vec = self.weighted_sum(w, V)\n",
        "            output.append(out_vec)\n",
        "\n",
        "        return output"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5036297e"
      },
      "source": [
        "## AttentionHead Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e6759f4"
      },
      "source": [
        "class AttentionHead:\n",
        "    def __init__(self, embed_dim, head_dim):\n",
        "        self.embed_dim = embed_dim\n",
        "        self.head_dim = head_dim\n",
        "\n",
        "        self.W_q = random_matrix((embed_dim, head_dim))\n",
        "        self.W_k = random_matrix((embed_dim, head_dim))\n",
        "        self.W_v = random_matrix((embed_dim, head_dim))\n",
        "\n",
        "    # -------------------------\n",
        "    # Copy these 5 methods below\n",
        "    # -------------------------\n",
        "    def dot(self, a, b):\n",
        "        s = 0\n",
        "        for i in range(len(a)):\n",
        "            s += a[i] * b[i]\n",
        "        return s\n",
        "\n",
        "    def attention_scores(self, Q, K):\n",
        "        matrix = []\n",
        "        for qi in Q:\n",
        "            row = []\n",
        "            for kj in K:\n",
        "                row.append(self.dot(qi, kj))\n",
        "            matrix.append(row)\n",
        "        return matrix\n",
        "\n",
        "    def soft_max(self, scores):\n",
        "        scores = np.array(scores, dtype=np.float64)   # convert to array\n",
        "        scores = scores - np.max(scores)             # numerical stability\n",
        "        exps = np.exp(scores)                         # numpy exp works on arrays\n",
        "        probs = exps / np.sum(exps, axis=-1, keepdims=True)\n",
        "        return probs\n",
        "\n",
        "    def weighted_sum(self, weights, V):\n",
        "        dim = len(V[0])\n",
        "        result = [0.0 for _ in range(dim)]\n",
        "\n",
        "        for i in range(len(V)):\n",
        "            for d in range(dim):\n",
        "                result[d] += weights[i] * V[i][d]\n",
        "        return result\n",
        "\n",
        "    def compute_attention(self, Q, K, V):\n",
        "        scores = self.attention_scores(Q, K)\n",
        "        for i in range(len(scores)):\n",
        "            for j in range(len(scores[i])):\n",
        "                scores[i][j] /= sqrt(self.head_dim)\n",
        "\n",
        "        output = []\n",
        "        for row in scores:\n",
        "            w = self.soft_max(row)\n",
        "            out_vec = self.weighted_sum(w, V)\n",
        "            output.append(out_vec)\n",
        "        return output\n",
        "\n",
        "    # -------------------------\n",
        "    # Main forward\n",
        "    # -------------------------\n",
        "    def forward(self, X):\n",
        "        Q, K, V = [], [], []\n",
        "        for token_vec in X:\n",
        "            Q.append(matmul(token_vec, self.W_q))\n",
        "            K.append(matmul(token_vec, self.W_k))\n",
        "            V.append(matmul(token_vec, self.W_v))\n",
        "\n",
        "        return self.compute_attention(Q, K, V)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8ea2580"
      },
      "source": [
        "## MultiHeadAttention Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1851d2f7"
      },
      "source": [
        "class MultiHeadAttention:\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        self.heads = [ AttentionHead(embed_dim, self.head_dim) for _ in range(num_heads) ]\n",
        "\n",
        "        # ðŸ”¥ Final linear layer after concatenation\n",
        "        self.Wo = random_matrix((embed_dim, embed_dim))\n",
        "\n",
        "    def concat_along_last_dim(self, head_outputs):\n",
        "        # head_outputs: list of [seq_len x head_dim] arrays\n",
        "        seq_len = len(head_outputs[0])\n",
        "        num_heads = len(head_outputs)\n",
        "        head_dim = len(head_outputs[0][0])\n",
        "\n",
        "        # initialize result\n",
        "        result = []\n",
        "\n",
        "        # loop over tokens\n",
        "        for i in range(seq_len):\n",
        "            concatenated = []\n",
        "            for head in head_outputs:\n",
        "                concatenated.extend(head[i])  # append head vector for token i\n",
        "            result.append(concatenated)\n",
        "\n",
        "        return result\n",
        "\n",
        "    def forward(self, X):\n",
        "        # X shape: [seq_len, embed_dim]\n",
        "\n",
        "        head_outputs = []\n",
        "\n",
        "        for head in self.heads:\n",
        "            # for each head, run attention\n",
        "            # BUT: X must first be projected down to head_dim\n",
        "\n",
        "            # Create Q,K,V using head weight matrices\n",
        "            # (selfattention already does this)\n",
        "\n",
        "            out = head.forward(X)\n",
        "            # shape: [seq_len, head_dim]\n",
        "\n",
        "            head_outputs.append(out)\n",
        "\n",
        "        # concatenate outputs from all heads\n",
        "        # final shape: [seq_len, embed_dim]\n",
        "        concatenated = self.concat_along_last_dim(head_outputs)\n",
        "\n",
        "        # final linear projection\n",
        "        final_output = []\n",
        "        for vec in concatenated:\n",
        "            final_output.append(matmul(vec, self.Wo))\n",
        "\n",
        "        return final_output"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8a4d632"
      },
      "source": [
        "## FeedForward and LayerNorm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2e18a03b"
      },
      "source": [
        "def zeros(n):\n",
        "    return [0.0 for _ in range(n)]\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(x, 0)\n",
        "\n",
        "\n",
        "def add_vectors(a, b):\n",
        "    \"\"\"Element-wise add two vectors (same length).\"\"\"\n",
        "    return [a[i] + b[i] for i in range(len(a))]\n",
        "def add_vectors_list(A, B):\n",
        "    return [add_vectors(A[i], B[i]) for i in range(len(A))]\n",
        "class FeedForward:\n",
        "    def __init__(self, embed_dim, hidden_dim):\n",
        "        self.W1 = np.random.uniform(-0.1, 0.1, (embed_dim, hidden_dim))\n",
        "        self.b1 = np.zeros(hidden_dim)\n",
        "\n",
        "        self.W2 = np.random.uniform(-0.1, 0.1, (hidden_dim, embed_dim))\n",
        "        self.b2 = np.zeros(embed_dim)\n",
        "\n",
        "\n",
        "    def forward(self, X):\n",
        "        # X shape: (B, T, D)\n",
        "        hidden = np.matmul(X, self.W1) + self.b1\n",
        "        hidden = np.maximum(hidden, 0)   # ReLU\n",
        "        out = np.matmul(hidden, self.W2) + self.b2\n",
        "        return out\n",
        "\n",
        "\n",
        "class LayerNorm:\n",
        "    def __init__(self, dim, eps=1e-5):\n",
        "        self.dim = dim\n",
        "        self.eps = eps\n",
        "        # Learnable parameters (gamma, beta)\n",
        "        self.gamma = [1.0] * dim\n",
        "        self.beta = [0.0] * dim\n",
        "\n",
        "    def forward(self, X):\n",
        "        # X is list of vectors: [seq_len][dim]\n",
        "        out = []\n",
        "        for vec in X:\n",
        "            mean = sum(vec) / self.dim\n",
        "            var = sum((v - mean)**2 for v in vec) / self.dim\n",
        "            std = (var + self.eps) ** 0.5\n",
        "\n",
        "            norm = [ (vec[i] - mean) / std for i in range(len(vec)) ]\n",
        "            out.append([ norm[i] * self.gamma[i] + self.beta[i] for i in range(len(vec)) ])\n",
        "\n",
        "        return out"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cf9bf4b"
      },
      "source": [
        "## TransformerBlock Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6c562d5c"
      },
      "source": [
        "class TransformerBlock:\n",
        "    def __init__(self, embed_dim, num_heads, ff_hidden_dim):\n",
        "        self.ln1 = LayerNorm(embed_dim)\n",
        "        self.mha = MultiHeadAttention(embed_dim, num_heads)\n",
        "        self.ln2 = LayerNorm(embed_dim)\n",
        "        self.ff = FeedForward(embed_dim, ff_hidden_dim)\n",
        "\n",
        "    def forward(self, X):\n",
        "        # if batch dim exists, drop it for simplicity\n",
        "        if isinstance(X, np.ndarray) and X.ndim == 3:\n",
        "            X = X[0].tolist()  # shape -> [seq_len, embed_dim]\n",
        "\n",
        "        # LayerNorm + MultiHeadAttention\n",
        "        normed = self.ln1.forward(X)\n",
        "        attn_out = self.mha.forward(normed)  # shape: [seq_len, embed_dim]\n",
        "\n",
        "        # Residual\n",
        "        x2 = add_vectors_list(X, attn_out)\n",
        "\n",
        "        # LayerNorm + FeedForward\n",
        "        normed2 = self.ln2.forward(x2)\n",
        "        ff_out = self.ff.forward(normed2)\n",
        "\n",
        "        # Residual\n",
        "        out = add_vectors_list(x2, ff_out)\n",
        "        return out"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac89fc71"
      },
      "source": [
        "## TransformerModel Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "840448c1"
      },
      "source": [
        "class TransformerModel:\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, ffn_hidden_dim, max_len=128):\n",
        "        self.token_embed = TokenEmbedding(vocab_size, embed_dim)\n",
        "        self.pos_embed = PosEmbedding(max_len, embed_dim)\n",
        "        self.blocks = [TransformerBlock(embed_dim, num_heads, ffn_hidden_dim)\n",
        "                       for _ in range(num_layers)]\n",
        "        self.Wo = random_matrix((embed_dim, vocab_size))\n",
        "        self.bo = np.zeros(vocab_size)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.max_len = max_len\n",
        "\n",
        "\n",
        "    def forward(self, ids):\n",
        "        # ids: list of token IDs\n",
        "        tok_vecs = self.token_embed.forward(ids)\n",
        "        pos_vecs = self.pos_embed.forward(ids)\n",
        "        X = [add_vectors(tok_vecs[i], pos_vecs[i]) for i in range(len(ids))]  # seq_len x embed_dim\n",
        "\n",
        "        # pass through blocks\n",
        "        for block in self.blocks:\n",
        "            X = block.forward(X)  # output: seq_len x embed_dim\n",
        "\n",
        "        # final linear layer\n",
        "        logits = np.matmul(X, self.Wo) + self.bo  # seq_len x vocab_size\n",
        "        return logits\n",
        "\n",
        "    def generate(self, idx, max_new_tokens, tokenizer):\n",
        "        \"\"\"\n",
        "        idx: list of token ids (context)\n",
        "        max_new_tokens: how many tokens to generate\n",
        "        tokenizer: your BPE or simple tokenizer\n",
        "        \"\"\"\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            # forward pass: logits shape (T, vocab)\n",
        "            logits = self.forward(idx)\n",
        "\n",
        "            # take last position\n",
        "            last_logits = logits[-1]   # shape: (vocab,)\n",
        "\n",
        "            # convert to probabilities using softmax\n",
        "            exps = np.exp(last_logits - np.max(last_logits))\n",
        "            probs = exps / np.sum(exps)\n",
        "\n",
        "            # sample from distribution (probabilistic)\n",
        "            next_id = int(np.random.choice(len(probs), p=probs))\n",
        "\n",
        "\n",
        "            # append prediction\n",
        "            idx.append(next_id)\n",
        "\n",
        "        return idx"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c280e69"
      },
      "source": [
        "## MiniTransformer Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abf4885c"
      },
      "source": [
        "class MiniTransformer:\n",
        "    def __init__(self, vocab_size, max_len, embed_dim, num_heads, ff_hidden_dim, num_layers):\n",
        "        self.token_embed = TokenEmbedding(vocab_size, embed_dim)\n",
        "        self.pos_embed = PosEmbedding(max_len, embed_dim)\n",
        "\n",
        "        self.blocks = [\n",
        "            TransformerBlock(embed_dim, num_heads, ff_hidden_dim)\n",
        "            for _ in range(num_layers)\n",
        "        ]\n",
        "\n",
        "    def forward(self, token_ids):\n",
        "        tok = self.token_embed.forward(token_ids)\n",
        "        pos = self.pos_embed.forward(token_ids)\n",
        "\n",
        "        X = add_vectors_list(tok, pos)\n",
        "\n",
        "        for block in self.blocks:\n",
        "            X = block.forward(X)\n",
        "\n",
        "        return X"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "782e94c4"
      },
      "source": [
        "## Example Usage: Tokenizer Training and Model Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ea64aa00"
      },
      "source": [
        "# Example tiny corpus\n",
        "text = \"hello world hello transformer model mini gpt\"\n",
        "\n",
        "# Initialize and train tokenizer\n",
        "tokenizer = BPETokenizer(vocab_size=50)\n",
        "tokenizer.train(text)\n",
        "\n",
        "# Convert text to token IDs\n",
        "token_ids = tokenizer.encode(text)  # e.g., [1, 5, 1, 20, 3, ...]\n",
        "seq_len = 4  # number of tokens in input\n",
        "X_train = []\n",
        "y_train = []\n",
        "\n",
        "for i in range(len(token_ids) - seq_len):\n",
        "    X_train.append(token_ids[i:i+seq_len])\n",
        "    y_train.append(token_ids[i+seq_len])\n",
        "\n",
        "X_train = np.array(X_train)  # shape: (num_samples, seq_len)\n",
        "y_train = np.array(y_train)  # shape: (num_samples,)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "952cd46a"
      },
      "source": [
        "## Model Initialization and Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf39f6e5",
        "outputId": "2b0508b2-cfbe-43ab-b030-3d9e25ebcece"
      },
      "source": [
        "lr = 0.01  # learning rate\n",
        "model = TransformerModel(vocab_size=tokenizer.vocab_size, embed_dim=16,\n",
        "                         num_heads=2, num_layers=2, ffn_hidden_dim=32)\n",
        "\n",
        "epochs = 100\n",
        "import numpy as np\n",
        "\n",
        "# --- Hyperparameters ---\n",
        "lr = 0.1        # learning rate\n",
        "epochs = 200\n",
        "seq_len = 5     # small sequence length for tiny corpus\n",
        "\n",
        "# Assume you already have:\n",
        "# model: TransformerModel instance\n",
        "# tokenizer: BPETokenizer instance\n",
        "# text: training text (string)\n",
        "\n",
        "# Encode the text into token IDs\n",
        "ids = tokenizer.encode(text)\n",
        "\n",
        "# Training loop (simple next-token prediction)\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0.0\n",
        "    for i in range(len(ids) - seq_len):\n",
        "        x_ids = ids[i:i+seq_len]\n",
        "        y_id = ids[i+seq_len]\n",
        "\n",
        "        logits = model.forward(x_ids)\n",
        "        pred = logits[-1]  # last token logits\n",
        "\n",
        "        # softmax & loss\n",
        "        exp_pred = np.exp(pred - np.max(pred))\n",
        "        probs = exp_pred / np.sum(exp_pred)\n",
        "        loss = -np.log(probs[y_id] + 1e-8)\n",
        "        total_loss += loss\n",
        "\n",
        "        # gradient for Wo & bo\n",
        "        grad = probs.copy()\n",
        "        grad[y_id] -= 1.0\n",
        "\n",
        "        # last hidden vector\n",
        "        X_block = model.blocks[-1].forward(\n",
        "            [add_vectors(model.token_embed.forward(x_ids)[-1],\n",
        "                         model.pos_embed.forward(x_ids)[-1])]\n",
        "        )\n",
        "        h = np.array(X_block[-1])\n",
        "\n",
        "        # update\n",
        "        model.Wo -= lr * np.outer(h, grad)\n",
        "        model.bo -= lr * grad\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch}, loss: {total_loss / len(ids):.4f}\")"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss: 1.1180\n",
            "Epoch 10, loss: 0.8537\n",
            "Epoch 20, loss: 0.6364\n",
            "Epoch 30, loss: 0.4838\n",
            "Epoch 40, loss: 0.3894\n",
            "Epoch 50, loss: 0.3332\n",
            "Epoch 60, loss: 0.2987\n",
            "Epoch 70, loss: 0.2763\n",
            "Epoch 80, loss: 0.2608\n",
            "Epoch 90, loss: 0.2494\n",
            "Epoch 100, loss: 0.2408\n",
            "Epoch 110, loss: 0.2340\n",
            "Epoch 120, loss: 0.2285\n",
            "Epoch 130, loss: 0.2238\n",
            "Epoch 140, loss: 0.2199\n",
            "Epoch 150, loss: 0.2164\n",
            "Epoch 160, loss: 0.2133\n",
            "Epoch 170, loss: 0.2106\n",
            "Epoch 180, loss: 0.2081\n",
            "Epoch 190, loss: 0.2058\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca5a984b"
      },
      "source": [
        "## Text Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79566418",
        "outputId": "0c2bd399-27b5-4332-bb55-66524be7873d"
      },
      "source": [
        "prompt = \"hello world\"\n",
        "ids = tokenizer.encode(prompt)\n",
        "generated_ids = model.generate(ids, max_new_tokens=20, tokenizer=tokenizer)\n",
        "print(tokenizer.decode(generated_ids))"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello world gpt gpt gpt gpt gpt mini gpt gpt gpt mini gpt gpt gpt mini mini mini gpt gpt gpt gpt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f45b1498"
      },
      "source": [
        "## Test Cases (Commented Out in Original Code)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8c993645",
        "outputId": "b574889e-430e-472e-fc1c-dad54bf712e0"
      },
      "source": [
        "'''Test cases for all the clasees and functions'''\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    X_knn = [\n",
        "        [1, 2],\n",
        "        [2, 3],\n",
        "        [3, 3],\n",
        "        [8, 7],\n",
        "        [9, 8],\n",
        "        [10, 8]\n",
        "    ]\n",
        "\n",
        "    y = [\"A\", \"A\", \"A\", \"B\", \"B\", \"B\"]\n",
        "\n",
        "    knn = KNN(k=3)\n",
        "    knn.fit(X_knn, y)\n",
        "\n",
        "    print(knn.predict([2, 2]))   # should give \"A\"\n",
        "    print(knn.predict([9, 7]))   # should give \"B\"\n",
        "\n",
        "\n",
        "    text = \"this is a test. this test is fun.\"\n",
        "    tokenizer = BPETokenizer(vocab_size=50)\n",
        "    tokenizer.train(text)\n",
        "    print(tokenizer.vocab)\n",
        "    print(tokenizer.merges)\n",
        "\n",
        "    text = \"this is a test\"\n",
        "    tokenizer = BPETokenizer(vocab_size=50)\n",
        "    tokenizer.train(text)\n",
        "\n",
        "    ids = tokenizer.encode(\"this is a test\")\n",
        "    print(ids)\n",
        "\n",
        "    decoded_text = tokenizer.decode(ids)\n",
        "    # print(decoded_text)\n",
        "\n",
        "\n",
        "\n",
        "    def add_vectors(a,b):\n",
        "        return [a[i] + b[i] for i in range(len(a))]\n",
        "\n",
        "    token_embed = TokenEmbedding(vocab_size=1000, embed_dim=32)\n",
        "    pos_embed = PosEmbedding(max_len=512, embed_dim=32)\n",
        "\n",
        "    ids = [8,6,4,11]\n",
        "\n",
        "    token_vectors = token_embed.forward(ids)\n",
        "    pos_vectors   = pos_embed.forward(ids)\n",
        "\n",
        "    final_vectors = [\n",
        "        add_vectors(token_vectors[i], pos_vectors[i])\n",
        "        for i in range(len(ids))\n",
        "    ]\n",
        "\n",
        "    print(final_vectors)\n",
        "\n",
        "    #Fake tiny example for SelfAttention\n",
        "    embed_dim = 4\n",
        "    X_attention_input = [\n",
        "        [0.1, 0.2, 0.3, 0.4],\n",
        "        [0.5, 0.4, 0.3, 0.2]\n",
        "    ]\n",
        "\n",
        "    att = SelfAttention(embed_dim)\n",
        "\n",
        "    # Convert X_attention_input to a numpy array with a batch dimension\n",
        "    # Expected shape for Q, K, V in SelfAttention.forward is (batch, seq_len, embed_dim)\n",
        "    X_np_for_attention = np.array(X_attention_input).reshape(1, len(X_attention_input), embed_dim)\n",
        "\n",
        "    # Pass X_np_for_attention for Q, K, and V as a simple test for self-attention\n",
        "    output_att, weights_att = att.forward(X_np_for_attention, X_np_for_attention, X_np_for_attention)\n",
        "\n",
        "    print(\"Output shape:\", output_att.shape)\n",
        "    print(\"Weights shape:\", weights_att.shape)\n",
        "\n",
        "    # ---- BUILD INPUT X ----\n",
        "    tokenizer = BPETokenizer(vocab_size=1000)\n",
        "    tokenizer.train(\"this is a test corpus for building tiny gpt tokenizer\")\n",
        "    token_embedding = TokenEmbedding(vocab_size=1000, embed_dim=4)\n",
        "    pos_embedding   = PosEmbedding(max_len=50, embed_dim=4)\n",
        "\n",
        "    ids = tokenizer.encode(\"this is a test\")\n",
        "\n",
        "    token_embed = token_embedding.forward(ids)\n",
        "    pos_embed = pos_embedding.forward(ids)\n",
        "\n",
        "    # Add token+pos embeddings\n",
        "    X_transformer_block = []\n",
        "    for i in range(len(token_embed)):\n",
        "        vec = []\n",
        "        for a, b in zip(token_embed[i], pos_embed[i]):\n",
        "            vec.append(a + b)\n",
        "        X_transformer_block.append(vec)\n",
        "\n",
        "    # ---- RUN TRANSFORMER BLOCK ----\n",
        "    tb = TransformerBlock(embed_dim=4, num_heads=2, ff_hidden_dim=16)\n",
        "    out = tb.forward(X_transformer_block)\n",
        "\n",
        "    print(\"block out shape:\", len(out), len(out[0]))\n",
        "    print(out)\n",
        "\n",
        "    print(\"block out shape:\", len(out), len(out[0]))\n",
        "    print(out)\n",
        "\n",
        "\n",
        "    print(\"Testing full TransformerBlock...\")\n",
        "    tb = TransformerBlock(embed_dim=4, num_heads=2, ff_hidden_dim=16)\n",
        "\n",
        "    out = tb.forward(X_transformer_block)\n",
        "    print(\"block out shape:\", len(out), len(out[0]))\n",
        "    print(out)\n",
        "\n",
        "    print(\"mha out shape:\", len(out), len(out[0]))  # expect seq_len x embed_dim\n",
        "\n",
        "    # small test X (seq_len=4, embed_dim must match your token/embed dims)\n",
        "\n",
        "\n",
        "    embed_dim = 4\n",
        "    num_heads = 2  # head_dim = 2\n",
        "    token_embedding = TokenEmbedding(vocab_size=100, embed_dim=embed_dim)\n",
        "    pos_embedding = PosEmbedding(max_len=20, embed_dim=embed_dim)\n",
        "    tokenizer = BPETokenizer(vocab_size=100)\n",
        "    tokenizer.train(\"this is a test\")   # small corpus ok\n",
        "\n",
        "    ids = tokenizer.encode(\"this is a test\")\n",
        "    token_vectors = token_embedding.forward(ids)\n",
        "    pos_vectors   = pos_embedding.forward(ids)\n",
        "    X_mha = [[a+b for a,b in zip(token_vectors[i], pos_vectors[i])] for i in range(len(ids))]\n",
        "\n",
        "    mha = MultiHeadAttention(embed_dim=embed_dim, num_heads=num_heads)\n",
        "    out = mha.forward(X_mha)\n",
        "    tb = TransformerBlock(embed_dim=4, num_heads=2, ff_hidden_dim=16)\n",
        "    out = tb.forward(X_mha)\n",
        "\n",
        "    model = TransformerModel(vocab_size=200, max_len=32, embed_dim=8, num_heads=2, num_layers=2, ffn_hidden_dim=32)\n",
        "    tokenizer = BPETokenizer(vocab_size=200)\n",
        "    tokenizer.train(\"this is a tiny corpus for testing\")\n",
        "    ids = tokenizer.encode(\"this is a test\")\n",
        "    logits = model.forward(ids)\n",
        "    print(\"logits shape:\", len(logits), len(logits[0]))   # expect seq_len x vocab_size\n",
        "\n",
        "    training_text = \"\"\"\n",
        "    hello world this is a tiny training dataset\n",
        "    hello there how are you\n",
        "    i am building a tiny transformer language model\n",
        "    \"\"\"\n",
        "\n",
        "    tokenizer = BPETokenizer(vocab_size=1000)\n",
        "    tokenizer.train(training_text)\n",
        "\n",
        "    # ACTUAL vocab size after training\n",
        "    vocab_size = len(tokenizer.vocab)\n",
        "\n",
        "    model = TransformerModel(vocab_size, embed_dim=64, num_heads=4, num_layers=2, ffn_hidden_dim=128)\n",
        "\n",
        "    prompt = \"hello\"\n",
        "    ids = tokenizer.encode(prompt)\n",
        "\n",
        "    generated = model.generate(ids, max_new_tokens=20, tokenizer=tokenizer)\n",
        "\n",
        "    print(tokenizer.decode(generated))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A\n",
            "B\n",
            "{'<unk>': 0, '<pad>': 1, ' ': 2, 't': 3, 'h': 4, 'i': 5, 's': 6, 'a': 7, 'e': 8, '.': 9, 'f': 10, 'u': 11, 'n': 12, 'is': 13, 'th': 14, 'this': 15, 'te': 16, 'tes': 17, 'test': 18, 'test.': 19, 'fu': 20, 'fun': 21, 'fun.': 22}\n",
            "[('i', 's'), ('t', 'h'), ('th', 'is'), ('t', 'e'), ('te', 's'), ('tes', 't'), ('test', '.'), ('f', 'u'), ('fu', 'n'), ('fun', '.')]\n",
            "[11, 9, 7, 14]\n",
            "[[0.003948731651882533, 0.014468110104772594, -0.005345825257313296, 0.002372116609630004, -0.0007182764152029505, 0.008617715566791128, -0.00568442831317727, -0.0011630307387571846, 0.0010140582388184632, 0.004079761358397082, -0.010270618883917913, -0.01774570425903446, 0.0015607427261459363, 0.00828397798860772, -0.008757249702976, -0.002111023511550871, 0.005889383066673776, -0.015665896534962295, 0.009818199774219395, -0.007663492467221191, 0.0015828285115975557, 0.0025632573613366353, 0.0026301368840841487, 0.004453749982770568, -0.0027336996367260486, 0.013749579261232934, -0.006894657910888408, 0.0016642868244182984, -4.694320310795995e-05, -0.0034493383563569463, 0.0026022131439894063, 0.0027071312322661193], [0.011881477642648214, -0.008454182947345294, -0.005198301822498699, -0.0066812675860202855, -0.015321391969007912, 0.009573322137272421, 0.00653928847220395, -0.014254154980316116, 0.0008447493408736493, -0.0016437216991670896, 0.013578511141195303, 0.004466079550095899, -0.00948787314463535, 0.004740853029122876, 0.0057394695275331635, -0.0029824822699083056, -0.0020287389978237026, 0.006465873096054588, 0.003241880190951431, 0.014745383126040406, 0.0029171118549567486, -0.00897918066451079, -0.008656853383299449, -0.0029647655781436712, -0.009662846198839451, -0.0038003691062113466, 0.010005941598035813, 0.0025906567477799233, -0.009105796452955233, -0.0018364318248018843, -0.003177982901850517, -0.010588844852577588], [0.014243334813240326, 0.014526092318962236, -0.013434037576290254, 0.004506439801017928, 0.011619443123474243, -0.006606131131851281, 0.005274363271116259, 0.00503473703136941, 0.0021809999334898057, 0.007752612671640362, 0.0012060388260681544, -0.00048814637739551074, -0.0013041580617509513, 0.002361172659089193, -0.012280019883757425, -0.018872989420454168, -0.000848252080720739, 0.0015678599173420326, -0.0036938791398762327, -0.007044956365182185, -0.016183306858961615, -0.003807741236723816, 0.013141468423462251, -0.0020620812358188717, -0.00022911437262399057, 0.010754042987694097, 0.0014871978949463158, 0.004520913605196951, 0.0021477086928855613, -0.0038713477047814283, -0.004752537319794365, -0.0005492861515060303], [0.006815581660595834, 0.001524772830503532, -0.01513878118050642, 0.00536284082884031, 0.007377893877036608, 0.0069567916673618, 0.01147612983858184, -0.0068816727894667855, 0.00125174643969409, -0.008410276990443503, -0.003503992321329509, -0.009182978832828335, -0.0015238766105352877, 0.01675860121494173, 0.007106484052614784, -0.0047158758931309065, 0.01584522551224356, 0.01939380284500301, -0.011147155832526113, 0.00785972072838877, -0.015907675146827876, 0.0066268129531587665, 0.013854441179289727, -0.007698492452829211, -0.013452124740416575, -0.00811026078551955, -0.013486071459928105, -0.003680509467977644, 0.0012279861121698744, -0.002689780500617203, -0.01752305960804347, -0.0074107352441817025]]\n",
            "Output shape: (1, 2, 4)\n",
            "Weights shape: (1, 2, 2)\n",
            "block out shape: 4 4\n",
            "[[np.float64(-0.008588380104192647), np.float64(0.015281889254604688), np.float64(0.03257796236722438), np.float64(0.010164896301341998)], [np.float64(0.00248202137449594), np.float64(0.024248091947450662), np.float64(0.02007855533667945), np.float64(0.005869223297783464)], [np.float64(-0.0011484950299658018), np.float64(0.032339803354121954), np.float64(0.024675927183354075), np.float64(-0.024204815280768767)], [np.float64(-0.004586441454586327), np.float64(0.008149514557448969), np.float64(3.5104779511843144e-05), np.float64(0.020480661119440597)]]\n",
            "block out shape: 4 4\n",
            "[[np.float64(-0.008588380104192647), np.float64(0.015281889254604688), np.float64(0.03257796236722438), np.float64(0.010164896301341998)], [np.float64(0.00248202137449594), np.float64(0.024248091947450662), np.float64(0.02007855533667945), np.float64(0.005869223297783464)], [np.float64(-0.0011484950299658018), np.float64(0.032339803354121954), np.float64(0.024675927183354075), np.float64(-0.024204815280768767)], [np.float64(-0.004586441454586327), np.float64(0.008149514557448969), np.float64(3.5104779511843144e-05), np.float64(0.020480661119440597)]]\n",
            "Testing full TransformerBlock...\n",
            "block out shape: 4 4\n",
            "[[np.float64(0.006128436872168235), np.float64(-0.06836564714143094), np.float64(0.004267379206768636), np.float64(-0.003123859346893059)], [np.float64(-0.010310530691194758), np.float64(-0.0615519669359761), np.float64(0.008500916733080968), np.float64(-0.006423640549991493)], [np.float64(-0.035487777492242405), np.float64(-0.016326130765817563), np.float64(0.030197736916170385), np.float64(-0.015936139131697544)], [np.float64(0.010555041341284542), np.float64(-0.016486494808372956), np.float64(-0.023315284657273845), np.float64(-0.014806125787840834)]]\n",
            "mha out shape: 4 4\n",
            "logits shape: 5 200\n",
            "hello r is tin he t ld are e l mode in languag dat langua l b e tiny dataset build\n"
          ]
        }
      ]
    }
  ]
}